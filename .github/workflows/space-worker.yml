name: Space Worker

on:
  workflow_dispatch:
    inputs:
      space_url:
        description: X or Twitter Space URL
        required: false
        type: string
        default: ""
      post_id:
        description: Existing WordPress post ID optional
        required: false
        type: string
        default: ""
      gcs_prefix:
        description: GCS prefix default spaces YYYY MM
        required: false
        type: string
        default: ""
      make_public:
        description: Make uploaded artifacts public
        required: false
        type: choice
        options: ["true","false"]
        default: "true"
      do_transcript:
        description: Generate transcript when captions are not available
        required: false
        type: choice
        options: ["true","false"]
        default: "true"
      mode:
        description: Limit processing
        required: false
        type: choice
        options: ["","transcript_only","attendees_only","replies_only"]
        default: ""
      existing_mp3_url:
        description: For transcript only provide URL to existing MP3
        required: false
        type: string
        default: ""
      aggressive_denoise:
        description: Use RNNoise arnndn denoiser
        required: false
        type: choice
        options: ["false","true"]
        default: "false"
      purple_tweet_url:
        description: Purple pill tweet URL optional
        required: false
        type: string
        default: ""
      audio_profile:
        description: Audio profile for encode (default 'radio')
        required: false
        type: choice
        options: ["transparent","radio","aggressive"]
        default: "radio"

permissions:
  contents: read
  packages: read

env:
  GCP_SA_KEY:       ${{ secrets.GCP_SA_KEY       || vars.GCP_SA_KEY }}
  GCS_BUCKET:       ${{ secrets.GCS_BUCKET       || vars.GCS_BUCKET }}
  WP_BASE_URL:      ${{ secrets.WP_BASE_URL      || secrets.WP_URL || vars.WP_BASE_URL || vars.WP_URL }}
  WP_USER:          ${{ secrets.WP_USER          || vars.WP_USER }}
  WP_APP_PASSWORD:  ${{ secrets.WP_APP_PASSWORD  || vars.WP_APP_PASSWORD }}
  DEEPGRAM_API_KEY: ${{ secrets.DEEPGRAM_API_KEY || vars.DEEPGRAM_API_KEY }}

  # Same creds you already use for the crawler (Bearer + cookies)
  TWITTER_AUTHORIZATION: ${{ secrets.TWITTER_AUTHORIZATION || secrets.X_BEARER     || vars.TWITTER_AUTHORIZATION || vars.X_BEARER }}
  TWITTER_AUTH_TOKEN:    ${{ secrets.TWITTER_AUTH_TOKEN    || secrets.X_AUTH_TOKEN || vars.TWITTER_AUTH_TOKEN    || vars.X_AUTH_TOKEN }}
  TWITTER_CSRF_TOKEN:    ${{ secrets.TWITTER_CSRF_TOKEN    || secrets.X_CSRF       || vars.TWITTER_CSRF_TOKEN    || vars.X_CSRF }}

  # Legacy API keys (not required for web replies, but kept for compatibility)
  TW_API_CONSUMER_KEY:        ${{ secrets.TW_API_CONSUMER_KEY        || vars.TW_API_CONSUMER_KEY }}
  TW_API_CONSUMER_SECRET:     ${{ secrets.TW_API_CONSUMER_SECRET     || vars.TW_API_CONSUMER_SECRET }}
  TW_API_ACCESS_TOKEN:        ${{ secrets.TW_API_ACCESS_TOKEN        || vars.TW_API_ACCESS_TOKEN }}
  TW_API_ACCESS_TOKEN_SECRET: ${{ secrets.TW_API_ACCESS_TOKEN_SECRET || vars.TW_API_ACCESS_TOKEN_SECRET }}

  WORKDIR: ${{ github.workspace }}/work
  ARTDIR:  ${{ github.workspace }}/out

jobs:
  process:
    name: Process Space
    runs-on: ubuntu-latest
    timeout-minutes: 180
    concurrency:
      group: ${{ format('space-worker-{0}-{1}', github.ref, github.event.inputs.post_id != '' && github.event.inputs.post_id || github.run_id) }}
      cancel-in-progress: false

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Notify WP queued
        if: ${{ env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' && github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euo pipefail
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" \
            -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/worker-status" \
            -d "$(jq -n --arg pid "${{ github.event.inputs.post_id }}" \
                       --arg status "queued" \
                       --arg msg "Workflow received and queued" \
                       --arg run "${{ github.run_id }}" \
                       --argjson progress 1 \
                       '{post_id: ($pid|tonumber), status:$status, message:$msg, run_id:$run, progress:$progress}')"

      - name: Install deps
        shell: bash
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends ffmpeg jq python3 python3-pip ca-certificates gnupg
          python3 -m pip install --upgrade pip
          python3 -m pip install --no-cache-dir yt-dlp python-twitter tldextract
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list
          curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk
          echo "${{ github.token }}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin || true

      - name: Validate config and prefixes
        id: cfg
        shell: bash
        run: |
          set -euxo pipefail
          test -n "${GCP_SA_KEY}" || { echo "GCP_SA_KEY missing"; exit 1; }
          test -n "${GCS_BUCKET}" || { echo "GCS_BUCKET missing"; exit 1; }
          mkdir -p "$WORKDIR" "$ARTDIR" "$ARTDIR/logs" ".github/workflows/scripts"
          PFX="$(echo "${{ github.event.inputs.gcs_prefix }}" | sed -E 's#^/*##; s#/*$##')"
          if [ -z "$PFX" ]; then PFX="spaces/$(date +%Y)/$(date +%m)"; fi
          echo "PREFIX=$PFX"                  >> "$GITHUB_ENV"
          echo "BUCKET_PREFIX=${PFX#spaces/}" >> "$GITHUB_ENV"

      - name: Derive Space ID and base
        id: ids
        shell: bash
        env:
          URL: ${{ github.event.inputs.space_url }}
        run: |
          set -euxo pipefail
          SID=""
          if [ -n "$URL" ]; then
            SID="$(echo "$URL" | sed -nE 's#^.*/i/spaces/([^/?#]+).*#\1#p')"
          fi
          [ -z "$SID" ] && SID="unknown"
          BASE="space-$(date +%m-%d-%Y)-${SID}"
          echo "SPACE_ID=${SID}" >> "$GITHUB_ENV"
          echo "BASE=${BASE}"    >> "$GITHUB_ENV"
          echo "space_id=${SID}" >> "$GITHUB_OUTPUT"
          echo "base=${BASE}"    >> "$GITHUB_OUTPUT"

      - name: GCP auth
        if: ${{ github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euxo pipefail
          printf '%s' "${GCP_SA_KEY}" > "${HOME}/gcp-key.json"
          gcloud auth activate-service-account --key-file="${HOME}/gcp-key.json" >/dev/null

      - name: X preflight auth
        id: x_preflight
        if: ${{ github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euo pipefail
          AUTH="${TWITTER_AUTHORIZATION:-}"
          AT="${TWITTER_AUTH_TOKEN:-}"
          CT="${TWITTER_CSRF_TOKEN:-}"
          if [ -n "$AUTH" ] && ! printf '%s' "$AUTH" | grep -q '^Bearer '; then AUTH=""; fi
          [ -n "${TWITTER_AUTHORIZATION:-}" ] && echo "::add-mask::${TWITTER_AUTHORIZATION}"
          [ -n "$AT" ] && echo "::add-mask::${AT}"
          [ -n "$CT" ] && echo "::add-mask::${CT}"
          OK=0; REASON="no_creds"
          [ -n "$AT" ] && [ -n "$CT" ] && OK=1 && REASON="cookie_ok" || true
          [ -n "$AUTH" ] && OK=1 && REASON="${REASON}_bearer_present" || true
          echo "ok=${OK}"         >> "$GITHUB_OUTPUT"
          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          [ -n "$AUTH" ] && echo "TWITTER_AUTHORIZATION=$AUTH" >> "$GITHUB_ENV"

      - name: Run crawler
        id: crawl
        if: ${{ github.event.inputs.mode != 'replies_only' && steps.x_preflight.outputs.ok == '1' }}
        shell: bash
        env:
          SID: ${{ steps.ids.outputs.space_id }}
        run: |
          set -euxo pipefail
          mkdir -p "${ARTDIR}" "${ARTDIR}/logs"
          docker pull ghcr.io/hitomarukonpaku/twspace-crawler:latest || true
          LOG_STD="${ARTDIR}/logs/crawler_${SID}.out.log"
          LOG_ERR="${ARTDIR}/logs/crawler_${SID}.err.log"
          set +e
          timeout 20m docker run --rm \
            -e TWITTER_AUTHORIZATION \
            -e TWITTER_AUTH_TOKEN \
            -e TWITTER_CSRF_TOKEN \
            -v "${ARTDIR}:/app/download" \
            -v "${ARTDIR}/logs:/app/logs" \
            ghcr.io/hitomarukonpaku/twspace-crawler:latest \
            --id "${SID}" --force > >(tee -a "$LOG_STD") 2> >(tee -a "$LOG_ERR" >&2)
          RC=$?
          set -e
          echo "crawler_exit=${RC}"
          AUDIO_FILE="$(find "${ARTDIR}" -type f \( -iname '*.m4a' -o -iname '*.mp3' -o -iname '*.mp4' -o -iname '*.aac' -o -iname '*.webm' -o -iname '*.ogg' -o -iname '*.wav' -o -iname '*.ts' \) -printf '%T@ %p\n' | sort -nr | head -n1 | cut -d' ' -f2- || true)"
          if [ -n "${AUDIO_FILE:-}" ] && [ -f "${AUDIO_FILE}" ]; then
            echo "INPUT_FILE=${AUDIO_FILE}" >> "$GITHUB_ENV"
            echo "audio_file=${AUDIO_FILE}" >> "$GITHUB_OUTPUT"
          fi
          RAW="$(grep -hF 'getAudioSpaceById |' "$LOG_STD" "$LOG_ERR" | tail -n1 || true)"
          if [ -z "$RAW" ]; then
            RAW="$(grep -hF 'getAudioSpaceByRestId |' "$LOG_STD" "$LOG_ERR" | tail -n1 || true)"
          fi
          if [ -n "$RAW" ]; then
            printf '%s\n' "$RAW" | awk -F'\\| ' '{print $NF}' > "${ARTDIR}/_as_line.json" || true
          fi
          [ -s "${ARTDIR}/_as_line.json" ] && echo "as_line=${ARTDIR}/_as_line.json" >> "$GITHUB_OUTPUT" || true
          CC_JSONL="$(find "${ARTDIR}" -type f \( -iname '*cc.jsonl' -o -iname '*caption*.jsonl' -o -iname '*captions*.jsonl' \) -print | head -n1 || true)"
          if [ -n "${CC_JSONL:-}" ]; then
            echo "CRAWLER_CC=${CC_JSONL}" >> "$GITHUB_ENV"
          fi

      - name: Fallback download via yt dlp
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && (steps.crawl.outputs.audio_file == '' || steps.crawl.outcome != 'success') && github.event.inputs.existing_mp3_url == '' && github.event.inputs.space_url != '' }}
        shell: bash
        working-directory: ${{ env.WORKDIR }}
        env:
          URL: ${{ github.event.inputs.space_url }}
        run: |
          set -euxo pipefail
          yt-dlp -o "%(title)s.%(ext)s" -f "bestaudio/best" "$URL"
          IN="$(ls -S | head -n1 || true)"
          test -f "$IN" || { echo "No file downloaded"; exit 1; }
          echo "INPUT_FILE=$PWD/$IN" >> "$GITHUB_ENV"

      - name: Use provided MP3 for transcript only
        if: ${{ github.event.inputs.mode == 'transcript_only' && github.event.inputs.existing_mp3_url != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          curl -L "${{ github.event.inputs.existing_mp3_url }}" -o "${ARTDIR}/${BASE}.mp3"
          echo "INPUT_FILE=${ARTDIR}/${BASE}.mp3" >> "$GITHUB_ENV"

      - name: Detect lead silence seconds
        id: detect
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.INPUT_FILE != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          LOG="${WORKDIR}/silence.log"
          ffmpeg -hide_banner -i "$INPUT_FILE" -af "silencedetect=noise=-45dB:d=1" -f null - 2> "$LOG" || true
          LEAD="$(awk '/silence_end/ {print $5; exit}' "$LOG" || true)"
          case "$LEAD" in ''|*[^0-9.]* ) LEAD="0.0" ;; esac
          echo "TRIM_LEAD=${LEAD}" >> "$GITHUB_ENV"
          echo "lead=${LEAD}"       >> "$GITHUB_OUTPUT"

      - name: Trim head and tail (RF64-safe)
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.INPUT_FILE != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          TRIM_WAV="${WORKDIR}/trim_${{ github.run_id }}.wav"
          ffmpeg -hide_banner -y -i "$INPUT_FILE" \
            -af "silenceremove=start_periods=1:start_silence=1:start_threshold=-45dB:detection=peak,areverse,silenceremove=start_periods=1:start_silence=1:start_threshold=-45dB:detection=peak,areverse" \
            -rf64 always -c:a pcm_s16le "$TRIM_WAV"
          echo "AUDIO_IN=${TRIM_WAV}" >> "$GITHUB_ENV"

      - name: Probe audio format
        id: probe
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.AUDIO_IN != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          J="$(ffprobe -v error -select_streams a:0 -show_entries stream=channels,sample_rate -of json "$AUDIO_IN")"
          CH=$(echo "$J" | jq -r '.streams[0].channels // 1')
          SR=$(echo "$J" | jq -r '.streams[0].sample_rate // "48000"')
          echo "SRC_CH=${CH}" >> "$GITHUB_ENV"
          echo "SRC_SR=${SR}" >> "$GITHUB_ENV"

      - name: Encode MP3 with selected profile
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.AUDIO_IN != '' }}
        shell: bash
        env:
          PROF: ${{ github.event.inputs.audio_profile != '' && github.event.inputs.audio_profile || 'radio' }}
        run: |
          set -euxo pipefail
          OUT="${ARTDIR}/${BASE}.mp3"
          CH="${SRC_CH:-1}"
          SR="${SRC_SR:-48000}"
          if [ "${PROF}" = "transparent" ]; then
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -map a:0 -c:a libmp3lame -q:a 0 -ar "$SR" -ac "$CH" "$OUT"
          elif [ "${PROF}" = "radio" ]; then
            PRE="highpass=f=60,lowpass=f=14000,afftdn=nr=4:nf=-28,deesser=i=0.12,acompressor=threshold=-18dB:ratio=2:attack=12:release=220:makeup=2"
            PASS1_JSON="${WORKDIR}/loudnorm1.json"
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:print_format=json" -f null - 2>"${WORKDIR}/pass1.log" || true
            awk '/^{/{f=1} f{print} /}/{f=0}' "${WORKDIR}/pass1.log" > "$PASS1_JSON" || true
            if jq -e . "$PASS1_JSON" >/dev/null 2>&1; then
              I=$(jq -r '.input_i // "-16"'  "$PASS1_JSON")
              TP=$(jq -r '.input_tp // "-1.5"' "$PASS1_JSON")
              LRA=$(jq -r '.input_lra // "11"' "$PASS1_JSON")
              TH=$(jq -r '.input_thresh // "-26"' "$PASS1_JSON")
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:measured_I=$I:measured_TP=$TP:measured_LRA=$LRA:measured_thresh=$TH:linear=true" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            else
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            fi
          else
            PRE="highpass=f=70,lowpass=f=11500,afftdn=nr=8:nf=-25,deesser=i=0.2,acompressor=threshold=-18dB:ratio=2.8:attack=8:release=200:makeup=3"
            if [ "${{ github.event.inputs.aggressive_denoise }}" = "true" ]; then
              M="${WORKDIR}/rnnoise.rnnn"
              curl -fsSL -o "$M" https://raw.githubusercontent.com/GregorR/rnnoise-models/master/heavyrnnoise.rnnn || true
              if [ -s "$M" ]; then PRE="arnndn=m=${M},${PRE}"; fi
            fi
            PASS1_JSON="${WORKDIR}/loudnorm1.json"
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:print_format=json" -f null - 2>"${WORKDIR}/pass1.log" || true
            awk '/^{/{f=1} f{print} /}/{f=0}' "${WORKDIR}/pass1.log" > "$PASS1_JSON" || true
            if jq -e . "$PASS1_JSON" >/dev/null 2>&1; then
              I=$(jq -r '.input_i // "-16"'  "$PASS1_JSON")
              TP=$(jq -r '.input_tp // "-1.5"' "$PASS1_JSON")
              LRA=$(jq -r '.input_lra // "11"' "$PASS1_JSON")
              TH=$(jq -r '.input_thresh // "-26"' "$PASS1_JSON")
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:measured_I=$I:measured_TP=$TP:measured_LRA=$LRA:measured_thresh=$TH:linear=true" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            else
              ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            fi
          fi
          echo "MP3_PATH=${OUT}" >> "$GITHUB_ENV"

      - name: Upload MP3 to GCS
        id: upload_mp3
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.MP3_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          DEST="gs://${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.mp3"
          RAW="https://storage.googleapis.com/${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.mp3"
          PROXY="https://media.chbmp.org/${PREFIX}/${BASE}.mp3"
          gsutil -m cp "${MP3_PATH}" "$DEST"
          if [ "${{ github.event.inputs.make_public }}" = "true" ]; then
            (gsutil acl ch -u AllUsers:R "$DEST" || gsutil iam ch allUsers:objectViewer "gs://${GCS_BUCKET}") || true
          fi
          echo "audio_raw=${RAW}"     >> "$GITHUB_OUTPUT"
          echo "audio_proxy=${PROXY}" >> "$GITHUB_OUTPUT"

      - name: Write helper scripts (gen_vtt, polish, replies)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p ".github/workflows/scripts"

          # ---------------- gen_vtt.py (normalized clocks, emoji-filter, reactions export) ----------------
          cat > ".github/workflows/scripts/gen_vtt.py" << 'PY'
          #!/usr/bin/env python3
          import json, os, re, html, sys
          from datetime import datetime, timezone

          artdir = os.environ.get("ARTDIR") or ""
          base   = os.environ.get("BASE") or ""
          src    = os.environ.get("CC_JSONL") or ""
          shift  = float(os.environ.get("SHIFT_SECS") or "0")

          if not (artdir and base and src and os.path.isfile(src)):
              os.makedirs(artdir, exist_ok=True)
              open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8").write("WEBVTT\n\n")
              open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8").write("")
              sys.exit(0)

          EMOJI_RE = re.compile("[" +
              "\U0001F1E6-\U0001F1FF" "\U0001F300-\U0001F5FF" "\U0001F600-\U0001F64F" "\U0001F680-\U0001F6FF" +
              "\U0001F700-\U0001F77F" "\U0001F780-\U0001F7FF" "\U0001F800-\U0001F8FF" "\U0001F900-\U0001F9FF" +
              "\U0001FA00-\U0001FAFF" "\u2600-\u26FF" "\u2700-\u27BF" + "]+", re.UNICODE)
          ONLY_PUNCT_SPACE = re.compile(r"^[\s\.,;:!?\-–—'\"“”‘’•·]+$")

          def is_emoji_only(s: str) -> bool:
              if not s or not s.strip(): return False
              t = ONLY_PUNCT_SPACE.sub("", s)
              t = EMOJI_RE.sub("", t)
              return len(t.strip()) == 0

          def parse_time_iso(s):
              if not s: return None
              s=s.strip()
              try:
                  if s.endswith('Z'): s=s[:-1]+'+00:00'
                  # normalize yyyy-mm-ddThh:mm:ss+0000 -> +00:00
                  if re.search(r'[+-]\d{4}$', s):
                      s=s[:-5]+s[-5:-2]+':'+s[-2:]
                  dt=datetime.fromisoformat(s)
                  if dt.tzinfo is None: dt=dt.replace(tzinfo=timezone.utc)
                  return dt.timestamp()
              except: return None

          def to_float_seconds(x):
              # int/float or string containing number; supports ms
              try:
                  if x is None: return None
                  f=float(x)
                  # if it's suspiciously large (milliseconds), scale down
                  return f/1000.0 if f>4_000_000 else f
              except: return None

          def first(*vals):
              for v in vals:
                  if v not in (None,""): return v
              return None

          def esc(s): return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

          # We build two time families:
          # - REL family (offset/startSec/startMs/start) => seconds relative to an origin within the space
          # - ABS family (timestamp/programDateTime) => epoch seconds
          # Later we normalize each item to a single REL timeline:
          #   rel = (has_REL ? REL : ABS - abs0)
          # Then subtract audio trim shift and clamp to >= 0.
          REL_KEYS  = ("offset","startSec","startMs","start")
          ABS_KEYS  = ("timestamp","programDateTime","ts")

          raw_items=[]
          reactions=[]
          abs_candidates=[]

          with open(src,'r',encoding='utf-8',errors='ignore') as f:
              for line in f:
                  line=line.strip()
                  if not line: continue
                  try:
                      obj=json.loads(line)
                  except:
                      continue

                  # Some crawlers nest JSON inside payload/body; unwrap best-effort
                  layer=None; sender={}
                  if isinstance(obj,dict) and isinstance(obj.get("payload"),str):
                      try:
                          pl=json.loads(obj["payload"])
                          if isinstance(pl,dict) and isinstance(pl.get("body"),str):
                              try:
                                  layer=json.loads(pl["body"])
                                  sender=pl.get("sender") or {}
                              except: layer=None
                      except: pass
                  else:
                      layer=obj

                  def push_item(rel_ts, abs_ts, txt, disp, uname, avatar):
                      if not txt: return
                      # Filter out pure reactions (emoji-only) from transcript; keep for reaction overlay
                      if is_emoji_only(txt):
                          if abs_ts is not None or rel_ts is not None:
                              reactions.append({
                                  "t_abs": abs_ts,
                                  "t_rel": rel_ts,
                                  "emoji": txt,
                                  "name":  (disp or uname or "") or "",
                                  "handle": (uname or "").lstrip("@"),
                                  "avatar": avatar or ""
                              })
                          return
                      raw_items.append({
                          "rel": rel_ts, "abs": abs_ts,
                          "text": txt,
                          "name": (disp or uname or "Speaker"),
                          "username": (uname or "").lstrip("@"),
                          "avatar": avatar or ""
                      })
                      if abs_ts is not None:
                          abs_candidates.append(abs_ts)

                  def pick_rel_abs(d):
                      # REL in seconds preference
                      rel=None
                      for k in REL_KEYS:
                          if k in d and d[k] not in (None,""):
                              rel = to_float_seconds(d[k])
                              if rel is not None: break
                      # ABS in epoch seconds
                      abs_ts=None
                      for k in ABS_KEYS:
                          if k in d and d[k] not in (None,""):
                              if k=="programDateTime":
                                  abs_ts = parse_time_iso(d[k]); 
                              else:
                                  abs_ts = to_float_seconds(d[k])
                              if abs_ts is not None: break
                      return rel, abs_ts

                  # Heuristic field extraction across variants
                  candidates=[]
                  if isinstance(layer,dict) and layer:
                      d=layer
                      # likely caption-like events
                      txt = first(d.get("body"), d.get("text"), d.get("caption"))
                      disp=first(d.get("displayName"), d.get("speaker_name"), d.get("speakerName"),
                                 (sender or {}).get("display_name"))
                      uname=first(d.get("username"), d.get("user_id"), (sender or {}).get("screen_name"))
                      avat=first((sender or {}).get("profile_image_url_https"),
                                 (sender or {}).get("profile_image_url"))
                      rel,abs_ts = pick_rel_abs(d)

                      # Some logs have type/int codes; skip known non-speech types if present
                      ttype = d.get("type")
                      if txt:
                          candidates.append((rel,abs_ts,txt,disp,uname,avat,ttype))

                  # Also try the top-level object (some dumps are flat)
                  if isinstance(obj,dict):
                      txt = first(obj.get("text"), obj.get("caption"), obj.get("payloadText"))
                      disp=first(obj.get("displayName"), obj.get("speaker"), obj.get("user"), obj.get("name"))
                      uname=first(obj.get("username"), obj.get("handle"), obj.get("screen_name"))
                      avat=first(obj.get("profile_image_url_https"), obj.get("profile_image_url"))
                      rel,abs_ts = pick_rel_abs(obj)
                      if txt:
                          candidates.append((rel,abs_ts,txt,disp,uname,avat,None))

                  for (rel,abs_ts,txt,disp,uname,avat,ttype) in candidates:
                      # Discard obvious non-speech signals beyond emoji-only (hashtags walls etc.)?
                      # Keep if there is at least one letter/number.
                      if not re.search(r"[A-Za-z0-9]", txt) and not is_emoji_only(txt):
                          continue
                      push_item(rel,abs_ts,txt,disp,uname,avat)

          if not raw_items:
              os.makedirs(artdir, exist_ok=True)
              open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8").write("WEBVTT\n\n")
              open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8").write("")
              # write empty reactions & start marker
              open(os.path.join(artdir,f"{base}_reactions.json"),"w",encoding="utf-8").write("[]")
              open(os.path.join(artdir,f"{base}.start.txt"),"w",encoding="utf-8").write("")
              sys.exit(0)

          # Normalize to a single relative clock per item:
          abs0 = min(abs_candidates) if abs_candidates else None
          norm=[]
          for it in raw_items:
              if it["rel"] is not None:
                  t = it["rel"]
              elif it["abs"] is not None and abs0 is not None:
                  t = it["abs"] - abs0
              else:
                  # last resort: place at end (unlikely)
                  t = 0.0
              t = max(0.0, t - shift)
              norm.append({ **it, "t": float(t) })

          # sort, stabilize, and create end times
          norm.sort(key=lambda x: x["t"])
          EPS=0.0005
          last=-1e9
          for u in norm:
              if u["t"] <= last: u["t"] = last + EPS
              last = u["t"]

          def fmt_ts(t):
              if t<0: t=0.0
              h=int(t//3600); m=int((t%3600)//60); s=t%60
              return f"{h:02d}:{m:02d}:{s:06.3f}"

          # Group/merge same-speaker segments that are close
          MERGE_GAP=3.0
          groups=[]; cur=None
          for u in norm:
              if cur and u["username"]==cur["username"] and u["name"]==cur["name"] and (u["t"]-cur["end"])<=MERGE_GAP:
                  sep = "" if re.search(r'[.!?]"?$', cur["text"]) else " "
                  cur["text"]=(cur["text"]+sep+u["text"]).strip()
                  cur["end"]=max(cur["end"], u["t"]+0.8)
              else:
                  cur={"name":u["name"],"username":u["username"],"avatar":u["avatar"],
                       "start":u["t"],"end":u["t"]+0.8,"text":u["text"]}
                  groups.append(cur)

          # Smooth & set reasonable end durations
          MIN_DUR=0.80; MAX_DUR=10.0; GUARD=0.020
          for i,g in enumerate(groups):
              if i+1<len(groups):
                  nxt=groups[i+1]["start"]
                  dur=max(MIN_DUR, min(MAX_DUR, (nxt-g["start"])-GUARD))
                  g["end"]=g["start"]+dur
              else:
                  words=max(1, len(g["text"].split()))
                  dur=max(MIN_DUR, min(MAX_DUR, 0.33*words+0.7))
                  g["end"]=g["start"]+dur

          # Write VTT
          os.makedirs(artdir, exist_ok=True)
          with open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8") as vf:
              vf.write("WEBVTT\n\n")
              for i,g in enumerate(groups,1):
                  vf.write(f"{i}\n{fmt_ts(g['start'])} --> {fmt_ts(g['end'])}\n")
                  vf.write(f"<v {esc(g['name'])}> {esc(g['text'])}\n\n")

          # Transcript HTML with follow-along
          css='''
          <style>
          .ss3k-transcript{font:15px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;
            max-height:70vh; overflow-y:auto; scroll-behavior:smooth; border:1px solid #e5e7eb; border-radius:12px; padding:6px}
          .ss3k-seg{display:flex;gap:10px;padding:8px 10px;border-radius:10px;margin:6px 0}
          .ss3k-seg.active{background:#eef6ff;outline:1px solid #bfdbfe}
          .ss3k-avatar{width:26px;height:26px;border-radius:50%;flex:0 0 26px;margin-top:3px;background:#e5e7eb}
          .ss3k-meta{font-size:12px;color:#64748b;margin-bottom:2px}
          .ss3k-name a{color:#0f172a;text-decoration:none}
          .ss3k-text{white-space:pre-wrap;word-break:break-word;cursor:pointer}
          </style>
          '''
          js='''
          <script>
          (function(){
            function time(s){return parseFloat(s||'0')||0}
            function within(t,seg){return t>=time(seg.dataset.start) && t<time(seg.dataset.end)}
            function bind(){
              var audio=document.getElementById('ss3k-audio')||document.querySelector('audio[data-ss3k-player]');
              var cont=document.querySelector('.ss3k-transcript'); if(!audio||!cont) return;
              var segs=[].slice.call(cont.querySelectorAll('.ss3k-seg')); var lastId="";
              function tick(){
                var t=audio.currentTime||0, found=null;
                for(var i=0;i<segs.length;i++){ if(within(t,segs[i])){found=segs[i];break;} }
                segs.forEach(function(s){ s.classList.toggle('active', s===found); });
                if(found){
                  var top = found.offsetTop - cont.offsetTop;
                  if (Math.abs(cont.scrollTop - top) > 6) cont.scrollTop = top;
                }
              }
              audio.addEventListener('timeupdate', tick);
              audio.addEventListener('seeked', tick);
              segs.forEach(function(s){
                s.addEventListener('click', function(){
                  audio.currentTime = time(s.dataset.start)+0.05; audio.play().catch(function(){});
                });
              });
              tick();
            }
            if(document.readyState!=="loading") bind(); else document.addEventListener('DOMContentLoaded', bind);
          })();
          </script>
          '''
          with open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8") as tf:
              tf.write(css); tf.write('<div class="ss3k-transcript">\\n')
              for i,g in enumerate(groups,1):
                  uname=(g["username"] or "").lstrip("@")
                  prof=f"https://x.com/{html.escape(uname, True)}" if uname else ""
                  avatar=g.get("avatar") or (f"https://unavatar.io/x/{html.escape(uname, True)}" if uname else "")
                  if avatar and prof:
                      avtag=f'<a href="{prof}" target="_blank" rel="noopener"><img class="ss3k-avatar" src="{html.escape(avatar, True)}" alt=""></a>'
                  elif avatar:
                      avtag=f'<img class="ss3k-avatar" src="{html.escape(avatar, True)}" alt="">'
                  else:
                      avtag='<div class="ss3k-avatar" aria-hidden="true"></div>'
                  name_html = f'<span class="ss3k-name"><strong>{html.escape(g["name"], True)}</strong></span>'
                  if prof:
                      name_html = f'<span class="ss3k-name"><a href="{prof}" target="_blank" rel="noopener"><strong>{html.escape(g["name"], True)}</strong></a></span>'
                  tf.write(f'<div class="ss3k-seg" id="seg-{i:04d}" data-start="{g["start"]:.3f}" data-end="{g["end"]:.3f}"')
                  if uname: tf.write(f' data-handle="@{html.escape(uname, True)}"')
                  tf.write('>')
                  tf.write(avtag)
                  tf.write('<div class="ss3k-body">')
                  tf.write(f'<div class="ss3k-meta">{name_html} · <time>{fmt_ts(g["start"])}</time>–<time>{fmt_ts(g["end"])}</time></div>')
                  tf.write(f'<div class="ss3k-text">{html.escape(g["text"], True)}</div>')
                  tf.write('</div></div>\\n')
              tf.write('</div>\\n'); tf.write(js)

          # Export reactions for UI overlays (hearts, etc.)
          # Normalize reaction times onto the same relative clock
          rx=[]
          if reactions:
              for r in reactions:
                  if r["t_rel"] is not None:
                      t=r["t_rel"]
                  elif r["t_abs"] is not None and abs0 is not None:
                      t=r["t_abs"]-abs0
                  else:
                      continue
                  t=max(0.0, t - shift)
                  rx.append({
                      "t": round(t,3),
                      "emoji": r["emoji"],
                      "name": r["name"],
                      "handle": r["handle"],
                      "avatar": r["avatar"]
                  })
          with open(os.path.join(artdir,f"{base}_reactions.json"),"w",encoding="utf-8") as rf:
              json.dump(rx, rf, ensure_ascii=False)

          # Also write a best-guess absolute start time for WP
          start_iso=""
          if abs_candidates:
              start_iso = datetime.fromtimestamp(min(abs_candidates), timezone.utc).isoformat(timespec='seconds').replace('+00:00','Z')
          with open(os.path.join(artdir,f"{base}.start.txt"),"w",encoding="utf-8") as sf:
              sf.write((start_iso or "") + "\\n")
          PY
          chmod +x ".github/workflows/scripts/gen_vtt.py"

          # ---------------- polish_transcript.py (your improved version) ----------------
          cat > ".github/workflows/scripts/polish_transcript.py" << 'PY'
          #!/usr/bin/env python3
          # Polishes the HTML transcript ONLY (does not touch VTT).
          # In:  $ARTDIR/$BASE_transcript.html
          # Out: $ARTDIR/$BASE_transcript_polished.html
          import os, re, html, json, time
          from typing import List, Tuple
          ARTDIR = os.environ.get("ARTDIR",".")
          BASE   = os.environ.get("BASE","space")
          INP    = os.path.join(ARTDIR, f"{BASE}_transcript.html")
          OUT    = os.path.join(ARTDIR, f"{BASE}_transcript_polished.html")
          REPORT = os.environ.get("TRANSCRIPT_REPORT_JSON", os.path.join(ARTDIR, f"{BASE}_transcript_polish_report.json"))
          USE_GEC = os.environ.get("TRANSCRIPT_GEC", "true").lower() in ("1","true","yes","on")
          GEC_MODEL = os.environ.get("TRANSCRIPT_GEC_MODEL", "prithivida/grammar_error_correcter_v1")
          GEC_MAX_LINES = int(os.environ.get("TRANSCRIPT_GEC_MAX_LINES", "10"))
          GEC_BATCH_SIZE = int(os.environ.get("TRANSCRIPT_GEC_BATCH", "4"))
          GEC_SELECT = os.environ.get("TRANSCRIPT_GEC_SELECT", "auto")
          GEC_LEN_MIN = float(os.environ.get("TRANSCRIPT_GEC_LEN_RATIO_MIN", "0.5"))
          GEC_LEN_MAX = float(os.environ.get("TRANSCRIPT_GEC_LEN_RATIO_MAX", "1.6"))
          STRIP_EMOJI      = os.environ.get("TRANSCRIPT_STRIP_EMOJI", "true").lower() in ("1","true","yes","on")
          DROP_EMOJI_ONLY  = os.environ.get("TRANSCRIPT_DROP_EMOJI_ONLY", "true").lower() in ("1","true","yes","on")
          SMART_QUOTES     = os.environ.get("TRANSCRIPT_SMART_QUOTES", "false").lower() in ("1","true","yes","on")
          if not os.path.exists(INP) or os.path.getsize(INP) == 0: raise SystemExit(0)
          t0 = time.time()
          raw_html = open(INP, "r", encoding="utf-8", errors="ignore").read()
          TEXT_NODE = re.compile(r'(<(?:div|span)\s+class="ss3k-text"[^>]*>)(.*?)(</(?:div|span)>)', re.S | re.I)
          URL_RE = re.compile(r"https?://[^\s<>\"]+")
          STAGE_DIR_RE = re.compile(r"(\[[^\]]{1,80}\]|\([^)]{1,80}\))")
          EMOJI_RE = re.compile("[" + "\U0001F1E6-\U0001F1FF" "\U0001F300-\U0001F5FF" "\U0001F600-\U0001F64F" "\U0001F680-\U0001F6FF" + "\U0001F700-\U0001F77F" "\U0001F780-\U0001F7FF" "\U0001F800-\U0001F8FF" "\U0001F900-\U0001F9FF" + "\U0001FA00-\U0001FAFF" "\u2600-\u26FF" "\u2700-\u27BF" + "]+", re.UNICODE)
          ONLY_PUNCT_SPACE = re.compile(r"^[\s\.,;:!?\-–—'\"“”‘’•·]+$")
          FILLER_WORDS = [r"uh+", r"um+", r"er+", r"ah+", r"mm+h*", r"hmm+", r"eh+", r"uh\-huh", r"uhhuh", r"uh\-uh", r"uhuh"]
          FILLER_PHRASES = [r"you\s+know", r"i\s+mean", r"kind\s+of", r"sort\s+of", r"you\s+see"]
          FILLERS_RE = re.compile(r"\b(?:" + "|".join(FILLER_WORDS + FILLER_PHRASES) + r")\b", re.I)
          STUTTER_RE = re.compile(r"\b([A-Za-z])(?:\s+\1\b){1,5}")
          REPEAT_RE  = re.compile(r"\b([A-Za-z]{2,})\b(?:\s+\1\b){1,4}", re.I)
          def sentence_case(s: str) -> str:
              s = re.sub(r"\bi\b", "I", s)
              def cap_first(m): return (m.group(1) or "") + m.group(2).upper()
              return re.sub(r"(^|\.\s+|\?\s+|!\s+)([a-z])", cap_first, s)
          def ensure_end_punct(s: str) -> str:
              t = s.rstrip()
              if not t: return s
              if t[-1] in ".!?\":)””’'»]>": return s
              if URL_RE.search(t[-80:]) or STAGE_DIR_RE.search(t[-1:]): return s
              if len(re.findall(r"\b\w+\b", t)) >= 6: return t + "."
              return s
          def match_case(repl: str, orig: str) -> str:
              if orig.isupper(): return repl.upper()
              if orig.islower(): return repl.lower()
              if orig[:1].isupper() and orig[1:].islower(): return repl.capitalize()
              return repl
          EGGCORNS=[(r"\byouth\s*[- ]\s*in\s*[- ]\s*asia\b","euthanasia"),(r"\bcould\s+of\b","could have"),(r"\bshould\s+of\b","should have"),(r"\bwould\s+of\b","would have"),(r"\bmute\s+point\b","moot point"),(r"\bfor\s+all\s+intensive\s+purposes\b","for all intents and purposes"),(r"\bcase\s+and\s+point\b","case in point"),(r"\bdeep\s+seeded\b","deep-seated"),(r"\bslight\s+of\s+hand\b","sleight of hand"),(r"\bescape\s+goat\b","scapegoat"),(r"\bbaited\s+breath\b","bated breath"),(r"\bpeaked\s+my\s+interest\b","piqued my interest"),(r"\bwet\s+your\s+appetite\b","whet your appetite"),(r"\btounge\b","tongue"),(r"\btow\s+the\s+line\b","toe the line"),(r"\bplay\s+it\s+by\s+year\b","play it by ear"),(r"\bfree\s+reign\b","free rein"),(r"\bold\s*[- ]?\s*timer'?s\s+disease\b","Alzheimer's disease"),(r"\bwreckless\s+driving\b","reckless driving"),(r"\bminus\s+well\b","might as well"),(r"\bfirst\s+come\s*,?\s*first\s+serve\b","first come, first served"),(r"\bnip\s+it\s+in\s+the\s+butt\b","nip it in the bud"),(r"\bhome\s+in\s+on\b","home in on"),(r"\bhone\s+in\s+on\b","home in on"),(r"\bchest\s*[- ]\s*of\s*[- ]\s*drawers\b","chest of drawers")]
          EGG_REPLACERS=[(re.compile(p,re.I),r) for p,r in EGGCORNS]
          PROPER_CASE=[(re.compile(r"\bsars\s*-\s*cov\s*-\s*2\b",re.I),"SARS-CoV-2"),(re.compile(r"\bsars\s*cov\s*2\b",re.I),"SARS-CoV-2"),(re.compile(r"\bcovid\s*-\s*19\b",re.I),"COVID-19"),(re.compile(r"\bcovid19\b",re.I),"COVID-19"),(re.compile(r"\bcovid\b",re.I),"COVID"),(re.compile(r"\bmrna\b",re.I),"mRNA"),(re.compile(r"\bc d c\b",re.I),"CDC"),(re.compile(r"\bf d a\b",re.I),"FDA")]
          def apply_eggcorns(s:str)->str:
              for rx,rep in EGG_REPLACERS:
                  s=rx.sub(lambda m: match_case(rep,m.group(0)), s)
              return s
          def apply_proper_case(s:str)->str:
              for rx,rep in PROPER_CASE:
                  s=rx.sub(rep,s)
              return s
          def protect_urls(s:str)->Tuple[str,List[str]]:
              urls=[]
              def stash(m):
                  urls.append(m.group(0)); return f"<<<URL{len(urls)-1}>>>"
              return URL_RE.sub(stash,s),urls
          def restore_urls(s:str,urls:List[str])->str:
              for i,u in enumerate(urls): s=s.replace(f"<<<URL{i}>>>",u)
              return s
          def protect_stage_dirs(s:str)->Tuple[str,List[str]]:
              boxes=[]
              def stash(m):
                  boxes.append(m.group(0)); return f"<<<STAGE{len(boxes)-1}>>>"
              return STAGE_DIR_RE.sub(stash,s),boxes
          def restore_stage_dirs(s:str,boxes:List[str])->str:
              for i,b in enumerate(boxes): s=s.replace(f"<<<STAGE{i}>>>",b)
              return s
          def strip_emoji(s:str)->str: return EMOJI_RE.sub("",s)
          def is_emoji_only(s:str)->bool:
              if not s.strip(): return False
              t = ONLY_PUNCT_SPACE.sub("", s); t = EMOJI_RE.sub("", t); return len(t.strip())==0
          def to_smart_quotes(s:str)->str:
              s=s.replace("---","—").replace("--","–")
              s=re.sub(r'(^|[\\s\\(\\[])\\\"', r'\\1“', s); s=re.sub(r'\\\"','”',s)
              s=re.sub(r"(^|[\\s\\(\\[])\\'", r"\\1‘", s); s=re.sub(r"\\'","’",s); return s
          def base_clean_pipeline(txt:str, stats)->str:
              if not txt.strip(): return txt
              txt,urls = protect_urls(txt)
              txt,boxes= protect_stage_dirs(txt)
              if DROP_EMOJI_ONLY and is_emoji_only(txt):
                  stats["emoji_dropped"] += 1; return ""
              if STRIP_EMOJI:
                  before=txt; txt=strip_emoji(txt); 
                  if txt!=before: stats["emoji_stripped"] += 1
              txt = FILLERS_RE.sub("", txt)
              txt = STUTTER_RE.sub(lambda m: m.group(1), txt)
              txt = REPEAT_RE.sub(lambda m: m.group(1), txt)
              txt = re.sub(r"\\s{2,}", " ", txt).strip()
              txt = re.sub(r"\\bi\\b", "I", txt)
              txt = re.sub(r"\\s+([,.;:!?])", r"\\1", txt)
              txt = re.sub(r"([,;:])([^\\s])", r"\\1 \\2", txt)
              txt = apply_eggcorns(txt); txt = apply_proper_case(txt)
              txt = sentence_case(txt); txt = ensure_end_punct(txt)
              if SMART_QUOTES: txt = to_smart_quotes(txt)
              txt = restore_stage_dirs(txt, boxes)
              txt = restore_urls(txt, urls)
              txt = txt.replace("<","&lt;").replace(">","&gt;")
              return txt
          spans=[]
          def _collect(m): spans.append(m.group(2)); return m.group(0)
          TEXT_NODE.sub(_collect, raw_html)
          stats={"total_nodes":len(spans),"emoji_stripped":0,"emoji_dropped":0,"gec_attempted":0,"gec_applied":0,"changed_nodes":0,"duration_sec":0.0}
          cleaned=[base_clean_pipeline(t,stats) for t in spans]
          def quality_score(s:str)->float:
              plain=html.unescape(s or ""); longness=min(len(plain),2000)/10.0
              no_punct=15.0 if plain and not re.search(r"[.!?]",plain) else 0.0
              mostly_lower=12.0 if (re.sub(r"[^A-Za-z]+","",plain).islower() and len(plain)>20) else 0.0
              many_commas=-3.0 if plain.count(",")>=3 else 0.0
              return longness+no_punct+mostly_lower+many_commas
          indices=list(range(len(cleaned)))
          if os.environ.get("TRANSCRIPT_GEC_SELECT","auto")=="first":
              candidate_idxs=[i for i in indices if (cleaned[i] or "").strip()][:GEC_MAX_LINES]
          else:
              scored=sorted([i for i in indices if (cleaned[i] or "").strip()],
                            key=(lambda i: len(cleaned[i])) if GEC_SELECT=="longest" else (lambda i: quality_score(cleaned[i])),
                            reverse=True)
              candidate_idxs=scored[:max(0, min(GEC_MAX_LINES, len(scored)))]
          def apply_gec(lines:List[str])->List[str]:
              try:
                  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM; import torch
              except Exception: return lines
              try:
                  tok=AutoTokenizer.from_pretrained(GEC_MODEL); mdl=AutoModelForSeq2SeqLM.from_pretrained(GEC_MODEL)
                  mdl.eval(); mdl.to("cpu")
              except Exception: return lines
              out=[]; buf=[]
              def flush():
                  nonlocal out,buf
                  if not buf: return
                  enc=tok(buf, return_tensors="pt", padding=True, truncation=True, max_length=512)
                  with torch.no_grad():
                      gen=mdl.generate(input_ids=enc["input_ids"], attention_mask=enc["attention_mask"],
                                       max_new_tokens=96, num_beams=4, length_penalty=1.0, early_stopping=True)
                  outs=tok.batch_decode(gen, skip_special_tokens=True)
                  for o in outs:
                      o=o.strip().replace("<","&lt;").replace(">","&gt;")
                      o=apply_eggcorns(o); o=apply_proper_case(o); o=ensure_end_punct(sentence_case(o))
                      out.append(o)
                  buf=[]
              for line in lines:
                  buf.append("gec: "+html.unescape(line))
                  if len(buf)>=GEC_BATCH_SIZE: flush()
              flush(); return out
          if USE_GEC and candidate_idxs:
              stats["gec_attempted"]=len(candidate_idxs)
              selected=[cleaned[i] for i in candidate_idxs]
              corrected=apply_gec(selected)
              for pos,idx in enumerate(candidate_idxs):
                  orig=cleaned[idx]; new=corrected[pos] if pos<len(corrected) else orig
                  if new.strip():
                      ratio=(len(new)+1)/(len(orig)+1)
                      if GEC_LEN_MIN <= ratio <= GEC_LEN_MAX and new!=orig:
                          cleaned[idx]=new; stats["gec_applied"]+=1
          it=iter(cleaned)
          def _replace(m):
              open_tag,old_text,close_tag=m.group(1),m.group(2),m.group(3)
              try: new_text=next(it)
              except StopIteration: new_text=old_text
              return f"{open_tag}{new_text}{close_tag}"
          polished_html=TEXT_NODE.sub(_replace, raw_html)
          polished_html=re.sub(r"\\n{3,}","\\n\\n",polished_html)
          with open(OUT,"w",encoding="utf-8") as f: f.write(polished_html)
          stats["changed_nodes"]=sum(1 for a,b in zip(spans, cleaned) if a!=b)
          stats["duration_sec"]=round(time.time()-t0,3)
          try: open(REPORT,"w",encoding="utf-8").write(json.dumps(stats,ensure_ascii=False,indent=2))
          except Exception: pass
          PY
          chmod +x ".github/workflows/scripts/polish_transcript.py"

          # ---------------- replies.py (time-sorted, nested, linkify, smart link names) ----------------
          cat > ".github/workflows/scripts/replies.py" << 'PY'
          #!/usr/bin/env python3
          import os, re, sys, json, html, time
          from collections import defaultdict, deque

          try:
              import tldextract
          except Exception:
              tldextract = None

          ARTDIR = os.environ.get("ARTDIR",".")
          BASE   = os.environ.get("BASE","space")
          PURPLE = (os.environ.get("PURPLE_TWEET_URL","") or "").strip()

          REPLIES_OUT = os.path.join(ARTDIR, f"{BASE}_replies.html")
          LINKS_OUT   = os.path.join(ARTDIR, f"{BASE}_links.html")

          def write_empty():
              open(REPLIES_OUT,"w",encoding="utf-8").write("")
              open(LINKS_OUT,"w",encoding="utf-8").write("")

          def esc(x): return html.escape(x or "")

          def http_json(url, method="GET", headers=None, data=None, timeout=30):
              import urllib.request
              req = urllib.request.Request(url=url, method=method)
              if headers:
                  for k, v in headers.items(): req.add_header(k, v)
              body = data.encode("utf-8") if isinstance(data, str) else data
              try:
                  with urllib.request.urlopen(req, data=body, timeout=timeout) as r:
                      txt = r.read().decode("utf-8", "ignore")
                      return json.loads(txt)
              except Exception:
                  return None

          def get_guest_token(bearer):
              if not bearer: return None
              headers = {"authorization": bearer, "content-type": "application/json", "user-agent": "Mozilla/5.0"}
              d = http_json("https://api.twitter.com/1.1/guest/activate.json", method="POST", headers=headers, data="{}")
              return (d or {}).get("guest_token")

          def expand_text_with_entities(text, entities):
              text = text or ""
              if not entities: 
                  return esc(text)
              urls = entities.get("urls") or []
              out = text
              urls_sorted = sorted(urls, key=lambda u: len(u.get("url","")), reverse=True)
              for u in urls_sorted:
                  short = u.get("url") or ""
                  expanded = u.get("expanded_url") or u.get("unwound_url") or short
                  if short and short in out: out = out.replace(short, expanded)
              out = re.sub(r'(https?://\S+)', r'<a href="\1" target="_blank" rel="noopener">\1</a>', esc(out))
              return out

          def add_link(bucket, url):
              if not url: return
              dom = url
              if tldextract:
                  try:
                      ext=tldextract.extract(url)
                      dom=".".join([p for p in [ext.domain, ext.suffix] if p])
                  except Exception:
                      pass
              bucket[dom].add(url)

          def name_for_url(url):
              # 2-3 word smart-ish names from domain + path
              try:
                  m=re.match(r"https?://([^/]+)(/[^?#]*)?", url)
                  host = m.group(1) if m else ""
                  path = (m.group(2) or "/").strip("/")
                  host_core = host.split(":")[0]
                  # domain hints
                  if "youtube." in host_core or "youtu.be" in host_core: return "YouTube"
                  if "substack" in host_core: return "Substack"
                  if "chbmp" in host_core: return "CHBMP"
                  if "x.com" in host_core or "twitter.com" in host_core: return "Tweet"
                  if "who.int" in host_core: return "WHO"
                  if "pubmed" in host_core or "nih.gov" in host_core: return "PubMed"
                  # path keywords
                  parts=[p for p in re.split(r"[-_/]+", path) if p and len(p)>2][:3]
                  parts=[re.sub(r'[^A-Za-z0-9]+','',p).title() for p in parts][:3]
                  if parts: return " ".join(parts[:3])
                  # fallback: domain brand
                  brand = host_core.split(".")
                  if len(brand)>1: brand=brand[-2]
                  else: brand=brand[0]
                  return brand.title()
              except Exception:
                  return "Link"

          def try_web_conversation(tid_str):
              bearer = (os.environ.get("TWITTER_AUTHORIZATION","") or "").strip()
              at     = (os.environ.get("TWITTER_AUTH_TOKEN","") or "").strip()
              ct0    = (os.environ.get("TWITTER_CSRF_TOKEN","") or "").strip()
              if not (bearer or (at and ct0)): return None

              headers = {"user-agent":"Mozilla/5.0","accept":"application/json, text/plain, */*"}
              if at and ct0:
                  headers.update({"authorization": bearer or "Bearer", "x-csrf-token": ct0, "cookie": f"auth_token={at}; ct0={ct0}"})
              elif bearer:
                  gt = get_guest_token(bearer)
                  if not gt: return None
                  headers.update({"authorization": bearer, "x-guest-token": gt})

              # v2 timeline conversation
              url = f"https://api.twitter.com/2/timeline/conversation/{tid_str}.json?tweet_mode=extended&include_ext_alt_text=true"
              data = http_json(url, headers=headers)
              if not data: return None
              tweets = (data.get("globalObjects", {}) or {}).get("tweets", {}) or {}
              users  = (data.get("globalObjects", {}) or {}).get("users", {}) or {}
              if not tweets: return None
              return tweets, users

          def parse_time(ts):
              try:
                  return time.mktime(time.strptime(ts, "%a %b %d %H:%M:%S %z %Y"))
              except Exception:
                  return 0.0

          def build_tree(tweets, users, root_id):
              # collect nodes and edges; keep only conversation_id == root
              nodes={}
              children=defaultdict(list)
              links_by_domain=defaultdict(set)

              for tid, tw in tweets.items():
                  conv=str(tw.get("conversation_id_str") or tw.get("conversation_id") or "")
                  if conv != root_id: continue
                  nodes[tid]=tw

              for tid, tw in nodes.items():
                  ent = tw.get("entities") or {}
                  for u in (ent.get("urls") or []):
                      add_link(links_by_domain, u.get("expanded_url") or u.get("unwound_url") or u.get("url"))

              # parent chain
              for tid, tw in nodes.items():
                  parent = tw.get("in_reply_to_status_id_str") or ""
                  if parent and parent in nodes:
                      children[parent].append(tid)
                  else:
                      # direct reply to root if parent==root
                      if parent == root_id:
                          children[root_id].append(tid)

              # sort children by created_at ascending
              for p in children:
                  children[p].sort(key=lambda k: parse_time(nodes[k].get("created_at","")))

              return nodes, children, links_by_domain

          def render_thread(nodes, children, users, root_id):
              def render_item(tid, depth=0):
                  t = nodes[tid]; uid=str(t.get("user_id_str") or "")
                  u = users.get(uid, {})
                  name   = u.get("name") or "User"
                  handle = u.get("screen_name") or ""
                  avatar = (u.get("profile_image_url_https") or u.get("profile_image_url") or "").replace("_normal.","_bigger.")
                  text   = expand_text_with_entities(t.get("full_text") or t.get("text") or "", t.get("entities") or {})
                  url    = f"https://x.com/{handle}/status/{tid}"
                  ts     = t.get("created_at","")
                  ts_short = ts
                  try:
                      ts_short = time.strftime("%Y-%m-%d %H:%M:%SZ", time.strptime(ts, "%a %b %d %H:%M:%S %z %Y"))
                  except Exception:
                      pass
                  kids   = children.get(tid, [])

                  head = (
                    f'<div class="ss3k-reply" style="margin-left:{depth*16}px">'
                    f'  <a href="https://x.com/{handle}" target="_blank" rel="noopener"><img class="ss3k-ravatar" src="{esc(avatar)}" alt=""></a>'
                    f'  <div class="ss3k-rcontent">'
                    f'    <div class="ss3k-rname"><a href="https://x.com/{handle}" target="_blank" rel="noopener">{esc(name)}</a>'
                    f'      <span class="ss3k-rhandle">@{esc(handle)}</span>'
                    f'      · <a href="{url}" target="_blank" rel="noopener" class="ss3k-rtime">{esc(ts_short)}</a>'
                    f'    </div>'
                    f'    <div class="ss3k-rtext">{text}</div>'
                    f'  </div>'
                    f'</div>'
                  )

                  if not kids:
                      return head
                  # collapsible for children
                  body = "".join(render_item(k, depth+1) for k in kids)
                  return head + f'<details class="ss3k-thread" style="margin-left:{depth*16}px"><summary>Show {len(kids)} repl{"y" if len(kids)==1 else "ies"}</summary>{body}</details>'

              roots = children.get(root_id, [])
              roots.sort(key=lambda k: parse_time(nodes[k].get("created_at","")))
              css = """
              <style>
              .ss3k-reply{display:flex;gap:10px;padding:10px 8px;border-bottom:1px solid #eee}
              .ss3k-ravatar{width:32px;height:32px;border-radius:50%}
              .ss3k-rname{font:600 14px/1.2 system-ui,-apple-system,Segoe UI,Roboto,Arial}
              .ss3k-rhandle,.ss3k-rtime{font:12px/1.2 system-ui,-apple-system,Segoe UI,Roboto,Arial;color:#64748b;text-decoration:none}
              .ss3k-rcontent{flex:1}
              .ss3k-rtext{margin-top:4px;font:14px/1.45 system-ui,-apple-system,Segoe UI,Roboto,Arial}
              details.ss3k-thread{margin:6px 0 8px 42px}
              details.ss3k-thread > summary{cursor:pointer;color:#2563eb}
              .ss3k-links h4{margin:14px 0 6px 0;font:600 14px/1.2 system-ui}
              .ss3k-links ul{margin:0 0 10px 16px;padding:0}
              .ss3k-links li{margin:2px 0}
              </style>
              """
              html_parts=[css] + [render_item(tid,0) for tid in roots]
              return "".join(html_parts)

          def main():
              if not PURPLE:
                  write_empty(); return
              m = re.search(r"/([^/]+)/status/(\d+)", PURPLE)
              if not m: write_empty(); return
              screen_name = m.group(1); tid_str = m.group(2)
              out = try_web_conversation(tid_str)
              if out is None:
                  write_empty(); return
              tweets, users = out
              nodes, children, links_by_domain = build_tree(tweets, users, tid_str)
              if not nodes:
                  write_empty(); return
              open(REPLIES_OUT,"w",encoding="utf-8").write(render_thread(nodes, children, users, tid_str))

              # smart link list (2–3 word names)
              lines=["<div class='ss3k-links'>"]
              for dom in sorted(links_by_domain):
                  lines.append(f"<h4>{esc(dom)}</h4>")
                  lines.append("<ul>")
                  for u in sorted(links_by_domain[dom]):
                      nm = name_for_url(u)
                      eu = esc(u)
                      lines.append(f'<li><a href="{eu}" target="_blank" rel="noopener">{esc(nm)}</a></li>')
                  lines.append("</ul>")
              lines.append("</div>")
              open(LINKS_OUT,"w",encoding="utf-8").write("\n".join(lines))
          if __name__ == "__main__": main()
          PY
          chmod +x ".github/workflows/scripts/replies.py"

      - name: Build VTT and transcript after trim
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' }}
        shell: bash
        env:
          CC_JSONL: ${{ env.CRAWLER_CC }}
          SHIFT_SECS: ${{ steps.detect.outputs.lead || '0' }}
        run: |
          set -euxo pipefail
          if [ -n "${CC_JSONL:-}" ] && [ -s "${CC_JSONL}" ]; then
            CC_JSONL="${CC_JSONL}" ARTDIR="${ARTDIR}" BASE="${BASE}" SHIFT_SECS="${SHIFT_SECS}" \
              python3 ".github/workflows/scripts/gen_vtt.py"
            [ -s "${ARTDIR}/${BASE}.vtt" ] && echo "VTT_PATH=${ARTDIR}/${BASE}.vtt" >> "$GITHUB_ENV" || true
            [ -s "${ARTDIR}/${BASE}_transcript.html" ] && echo "TRANSCRIPT_PATH=${ARTDIR}/${BASE}_transcript.html" >> "$GITHUB_ENV" || true
          fi

      - name: Polish transcript (optional)
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.TRANSCRIPT_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          python3 ".github/workflows/scripts/polish_transcript.py" || true
          if [ -s "${ARTDIR}/${BASE}_transcript_polished.html" ]; then
            echo "TRANSCRIPT_PATH=${ARTDIR}/${BASE}_transcript_polished.html" >> "$GITHUB_ENV"
          fi

      - name: VTT via Deepgram fallback
        id: deepgram
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.VTT_PATH == '' && env.DEEPGRAM_API_KEY != '' && github.event.inputs.do_transcript == 'true' && env.MP3_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          curl -sS -X POST \
            -H "Authorization: Token ${DEEPGRAM_API_KEY}" \
            -H "Content-Type: audio/mpeg" \
            --data-binary @"${MP3_PATH}" \
            "https://api.deepgram.com/v1/listen?model=nova-2&smart_format=true&punctuate=true&format=vtt" \
            -o "${ARTDIR}/${BASE}.vtt" || true
          [ -s "${ARTDIR}/${BASE}.vtt" ] && echo "VTT_PATH=${ARTDIR}/${BASE}.vtt" >> "$GITHUB_ENV" || true

      - name: Upload VTT to GCS
        id: upload_vtt
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.VTT_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          DEST="gs://${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.vtt"
          RAW="https://storage.googleapis.com/${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.vtt"
          PROXY="https://media.chbmp.org/${PREFIX}/${BASE}.vtt"
          gsutil -m cp "${VTT_PATH}" "$DEST"
          if [ "${{ github.event.inputs.make_public }}" = "true" ]; then
            (gsutil acl ch -u AllUsers:R "$DEST" || gsutil iam ch allUsers:objectViewer "gs://${GCS_BUCKET}") || true
          fi
          echo "vtt_raw=${RAW}"     >> "$GITHUB_OUTPUT"
          echo "vtt_proxy=${PROXY}" >> "$GITHUB_OUTPUT"

      - name: Build attendees HTML
        id: attendees
        if: ${{ github.event.inputs.mode != 'replies_only' && steps.crawl.outcome == 'success' && steps.crawl.outputs.as_line != '' }}
        shell: bash
        env:
          CAND: ${{ steps.crawl.outputs.as_line }}
        run: |
          set -euxo pipefail
          OUT_JSON="${ARTDIR}/attendees.json"
          OUT_HTML="${ARTDIR}/attendees.html"
          jq -r '
            def mkp:
              { handle: (.twitter_screen_name // .user_results?.result?.legacy?.screen_name),
                name:   (.display_name       // .user_results?.result?.legacy?.name)
              }
              | select(.handle!=null and .handle!="" )
              | . + { url: ("https://x.com/" + .handle) };
            (.audioSpace // .) as $a
            | ($a.metadata?.creator_results?.result?.legacy?) as $h
            | ($h.screen_name // empty) as $H
            | {
                host:    ( if $H != "" then [ {handle:$H, name:($h.name // ""), url:("https://x.com/" + $H)} ] else [] end ),
                cohosts: ( ($a.participants?.admins   // []) | map(mkp) | map(select(.handle != $H)) | unique_by(.handle) ),
                speakers:( ($a.participants?.speakers // []) | map(mkp) | unique_by(.handle) )
              }
          ' "${CAND}" > "$OUT_JSON" || true
          if [ -s "$OUT_JSON" ]; then
            jq -r '
              def li: "  <li><a href=\"" + (.url//"#") + "\">" + ((.name // "") + " (@" + (.handle // "") + ")") + "</a></li>";
              def section(title; items):
                if (items|length) > 0
                then "<h3>" + title + "</h3>\n<ul>\n" + (items|map(li)|join("\n")) + "\n</ul>\n"
                else ""
                end;
              . as $d
              | section("Host"; $d.host)
              + section( (if ($d.cohosts|length)==1 then "Co-host" else "Co-hosts" end); $d.cohosts)
              + section("Speakers"; $d.speakers)
            ' "$OUT_JSON" > "$OUT_HTML"
            if grep -qi '<li><a ' "$OUT_HTML"; then
              echo "ATTN_HTML=${OUT_HTML}" >> "$GITHUB_ENV"
              echo "ATTENDEES_OK=1"       >> "$GITHUB_ENV"
            fi
          fi

      - name: Scrape replies and shared links (web)
        shell: bash
        env:
          PURPLE_TWEET_URL: ${{ github.event.inputs.purple_tweet_url }}
        run: |
          set -euo pipefail
          python3 ".github/workflows/scripts/replies.py" || true
          if [ -s "${ARTDIR}/${BASE}_replies.html" ]; then
            echo "REPLIES_PATH=${ARTDIR}/${BASE}_replies.html" >> "$GITHUB_ENV"
          fi
          if [ -s "${ARTDIR}/${BASE}_links.html" ]; then
            echo "LINKS_PATH=${ARTDIR}/${BASE}_links.html" >> "$GITHUB_ENV"
          fi

      - name: Derive Space title and publish date
        id: meta
        shell: bash
        env:
          AS_LINE: ${{ steps.crawl.outputs.as_line }}
        run: |
          set -euo pipefail
          TTL=""
          if [ -n "${AS_LINE:-}" ] && [ -s "${AS_LINE}" ]; then
            TTL="$(jq -r '(.audioSpace // .) as $a | ($a.metadata.title // $a.metadata.name // .title // "")' "${AS_LINE}" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
          fi
          if [ -z "$TTL" ]; then TTL="${BASE}"; fi
          echo "TTL_FINAL=$TTL" >> "$GITHUB_ENV"

          # Prefer crawler metadata started_at; else transcript start; else now
          START_ISO=""
          if [ -n "${AS_LINE:-}" ] && [ -s "${AS_LINE}" ]; then
            MS="$(jq -r '(.audioSpace // .) as $a | ($a.metadata.started_at // $a.metadata.created_at // $a.metadata.start // empty)' "${AS_LINE}")" || true
            if [[ "$MS" =~ ^[0-9]+$ ]]; then
              if [ ${#MS} -gt 10 ]; then SECS=$((MS/1000)); else SECS=$MS; fi
              START_ISO="$(date -u -d "@$SECS" +%Y-%m-%dT%H:%M:%SZ || true)"
            fi
          fi
          if [ -z "$START_ISO" ] && [ -s "${ARTDIR}/${BASE}.start.txt" ]; then
            START_ISO="$(head -n1 "${ARTDIR}/${BASE}.start.txt" | tr -d '\r\n')"
          fi
          if [ -z "$START_ISO" ]; then START_ISO="$(date -u +%Y-%m-%dT%H:%M:%SZ)"; fi
          echo "START_ISO=$START_ISO" >> "$GITHUB_ENV"

      - name: Register assets in WP
        if: ${{ github.event.inputs.mode == '' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' && (steps.upload_mp3.outputs.audio_proxy != '' || steps.upload_mp3.outputs.audio_raw != '') }}
        shell: bash
        env:
          PID:  ${{ github.event.inputs.post_id }}
          AUD:  ${{ steps.upload_mp3.outputs.audio_proxy || steps.upload_mp3.outputs.audio_raw }}
          VTTU: ${{ steps.upload_vtt.outputs.vtt_proxy   || steps.upload_vtt.outputs.vtt_raw }}
        run: |
          set -euo pipefail
          TTL="${TTL_FINAL:-${BASE}}"

          ATH_FILE="${WORKDIR}/empty_attendees.html"; : > "$ATH_FILE"
          [ -n "${ATTN_HTML:-}" ] && [ -s "${ATTN_HTML:-}" ] && ATH_FILE="${ATTN_HTML}"

          TR_FILE="${WORKDIR}/empty_transcript.html"; : > "$TR_FILE"
          [ -n "${TRANSCRIPT_PATH:-}" ] && [ -s "${TRANSCRIPT_PATH}" ] && TR_FILE="${TRANSCRIPT_PATH}"

          REP_FILE="${WORKDIR}/empty_replies.html"; : > "$REP_FILE"
          [ -n "${REPLIES_PATH:-}" ] && [ -s "${REPLIES_PATH}" ] && REP_FILE="${REPLIES_PATH}"

          LNK_FILE="${WORKDIR}/empty_links.html"; : > "$LNK_FILE"
          [ -n "${LINKS_PATH:-}" ] && [ -s "${LINKS_PATH}" ] && LNK_FILE="${LINKS_PATH}"

          # NEW: reactions JSON (emoji overlay events)
          RX_FILE="${WORKDIR}/empty_reactions.json"; echo "[]" > "$RX_FILE"
          if [ -s "${ARTDIR}/${BASE}_reactions.json" ]; then
            RX_FILE="${ARTDIR}/${BASE}_reactions.json"
          fi

          REQ="${WORKDIR}/wp_register_body.json"
          jq -n \
            --arg gcs   "${AUD}" \
            --arg mime  "audio/mpeg" \
            --arg pid   "${PID}" \
            --arg ttl   "${TTL}" \
            --arg vtt   "${VTTU}" \
            --arg when  "${START_ISO:-}" \
            --rawfile ath "${ATH_FILE}" \
            --rawfile tr  "${TR_FILE}" \
            --rawfile rep "${REP_FILE}" \
            --rawfile lnk "${LNK_FILE}" \
            --rawfile rx  "${RX_FILE}" \
            '{
               gcs_url: $gcs,
               mime:    $mime,
               post_id: ($pid|tonumber),
               title:   $ttl
             }
             + (if ($when|length)>0 then {post_date_gmt:$when, space_started_at:$when, publish_date:$when} else {} end)
             + (if ($vtt|length)>0 then {vtt_url:$vtt} else {} end)
             + (if ($ath|gsub("\\s";"")|length)>0 then {attendees_html:$ath} else {} end)
             + (if ($tr|gsub("\\s";"")|length)>0 then {transcript:$tr, has_transcript:true} else {} end)
             + (if ($rep|gsub("\\s";"")|length)>0 then {ss3k_replies_html:$rep} else {} end)
             + (if ($lnk|gsub("\\s";"")|length)>0 then {shared_links_html:$lnk} else {} end)
             # NEW: ship reactions json as a string field your WP can parse
             + (if ($rx|gsub("\\s";"")|length)>2 then {reactions_json:$rx} else {} end)
            ' > "$REQ"

          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" \
            -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/register" \
            --data-binary @"$REQ" | jq -r .


      - name: Patch WP attendees only
        if: ${{ github.event.inputs.mode == 'attendees_only' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' }}
        shell: bash
        run: |
          set -euo pipefail
          AT_HTML=""
          if [ -n "${ATTN_HTML:-}" ] && [ -s "${ATTN_HTML:-}" ]; then AT_HTML="$(cat "${ATTN_HTML}")"; fi
          BODY="$(jq -n --arg pid "${{ github.event.inputs.post_id }}" --arg ath "${AT_HTML}" \
            '{post_id: ($pid|tonumber), status:"complete", progress:100}
             + (if ($ath|length)>0 then {attendees_html:$ath} else {} end)')"
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/patch-assets" -d "$BODY" | jq -r .

      - name: Patch WP replies only
        if: ${{ github.event.inputs.mode == 'replies_only' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' }}
        shell: bash
        run: |
          set -euo pipefail
          REP="$( [ -n "${REPLIES_PATH:-}" ] && [ -s "${REPLIES_PATH:-}" ] && cat "${REPLIES_PATH}" || echo "" )"
          LNK="$( [ -n "${LINKS_PATH:-}" ] && [ -s "${LINKS_PATH:-}" ] && cat "${LINKS_PATH}" || echo "" )"
          BODY="$(jq -n --arg pid "${{ github.event.inputs.post_id }}" --arg rep "${REP}" --arg lnk "${LNK}" \
            '{post_id: ($pid|tonumber), status:"complete", progress:100}
             + (if ($rep|length)>0 then {ss3k_replies_html:$rep} else {} end)
             + (if ($lnk|length)>0 then {shared_links_html:$lnk} else {} end)')"
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/patch-assets" -d "$BODY" | jq -r .

      - name: Summary
        shell: bash
        env:
          SID: ${{ steps.ids.outputs.space_id }}
        run: |
          {
            echo "### Space Worker Summary"
            echo "- Mode ${{ github.event.inputs.mode }}"
            echo "- Space URL ${{ github.event.inputs.space_url }}"
            echo "- Purple URL ${{ github.event.inputs.purple_tweet_url }}"
            echo "- Space ID ${SID}"
            echo "- Post ID ${{ github.event.inputs.post_id }}"
            echo "- Title ${TTL_FINAL:-}"
            echo "- Publish (UTC) ${START_ISO:-}"
            if [ -n "${{ steps.upload_mp3.outputs.audio_proxy }}" ]; then
              echo "- Audio ${{ steps.upload_mp3.outputs.audio_proxy }}"
            elif [ -n "${{ steps.upload_mp3.outputs.audio_raw }}" ]; then
              echo "- Audio ${{ steps.upload_mp3.outputs.audio_raw }}"
            fi
            if [ -n "${{ steps.upload_vtt.outputs.vtt_proxy }}" ]; then
              echo "- VTT ${{ steps.upload_vtt.outputs.vtt_proxy }}"
            elif [ -n "${{ steps.upload_vtt.outputs.vtt_raw }}" ]; then
              echo "- VTT ${{ steps.upload_vtt.outputs.vtt_raw }}"
            fi
          } >> "$GITHUB_STEP_SUMMARY"
