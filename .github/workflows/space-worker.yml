name: Space Worker

on:
  workflow_dispatch:
    inputs:
      space_url:
        description: X or Twitter Space URL
        required: false
        type: string
        default: ""
      post_id:
        description: Existing WordPress post ID optional
        required: false
        type: string
        default: ""
      gcs_prefix:
        description: GCS prefix default spaces YYYY MM
        required: false
        type: string
        default: ""
      make_public:
        description: Make uploaded artifacts public
        required: false
        type: choice
        options: ["true","false"]
        default: "true"
      do_transcript:
        description: Generate transcript when captions are not available
        required: false
        type: choice
        options: ["true","false"]
        default: "true"
      mode:
        description: Limit processing
        required: false
        type: choice
        options: ["","transcript_only","attendees_only","replies_only"]
        default: ""
      existing_mp3_url:
        description: For transcript only provide URL to existing MP3
        required: false
        type: string
        default: ""
      aggressive_denoise:
        description: Use RNNoise arnndn denoiser
        required: false
        type: choice
        options: ["false","true"]
        default: "false"
      purple_tweet_url:
        description: Purple pill tweet URL optional
        required: false
        type: string
        default: ""
      audio_profile:
        description: Audio profile for encode (default 'radio')
        required: false
        type: choice
        options: ["transparent","radio","aggressive"]
        default: "radio"

permissions:
  contents: read
  packages: read

env:
  GCP_SA_KEY:       ${{ secrets.GCP_SA_KEY       || vars.GCP_SA_KEY }}
  GCS_BUCKET:       ${{ secrets.GCS_BUCKET       || vars.GCS_BUCKET }}
  WP_BASE_URL:      ${{ secrets.WP_BASE_URL      || secrets.WP_URL || vars.WP_BASE_URL || vars.WP_URL }}
  WP_USER:          ${{ secrets.WP_USER          || vars.WP_USER }}
  WP_APP_PASSWORD:  ${{ secrets.WP_APP_PASSWORD  || vars.WP_APP_PASSWORD }}
  DEEPGRAM_API_KEY: ${{ secrets.DEEPGRAM_API_KEY || vars.DEEPGRAM_API_KEY }}

  # Same creds you already use for the crawler (Bearer + cookies)
  TWITTER_AUTHORIZATION: ${{ secrets.TWITTER_AUTHORIZATION || secrets.X_BEARER     || vars.TWITTER_AUTHORIZATION || vars.X_BEARER }}
  TWITTER_AUTH_TOKEN:    ${{ secrets.TWITTER_AUTH_TOKEN    || secrets.X_AUTH_TOKEN || vars.TWITTER_AUTH_TOKEN    || vars.X_AUTH_TOKEN }}
  TWITTER_CSRF_TOKEN:    ${{ secrets.TWITTER_CSRF_TOKEN    || secrets.X_CSRF       || vars.TWITTER_CSRF_TOKEN    || vars.X_CSRF }}

  # Legacy API keys (not required for web replies, but kept for compatibility)
  TW_API_CONSUMER_KEY:        ${{ secrets.TW_API_CONSUMER_KEY        || vars.TW_API_CONSUMER_KEY }}
  TW_API_CONSUMER_SECRET:     ${{ secrets.TW_API_CONSUMER_SECRET     || vars.TW_API_CONSUMER_SECRET }}
  TW_API_ACCESS_TOKEN:        ${{ secrets.TW_API_ACCESS_TOKEN        || vars.TW_API_ACCESS_TOKEN }}
  TW_API_ACCESS_TOKEN_SECRET: ${{ secrets.TW_API_ACCESS_TOKEN_SECRET || vars.TW_API_ACCESS_TOKEN_SECRET }}

  WORKDIR: ${{ github.workspace }}/work
  ARTDIR:  ${{ github.workspace }}/out

jobs:
  process:
    name: Process Space
    runs-on: ubuntu-latest
    timeout-minutes: 180
    concurrency:
      group: ${{ format('space-worker-{0}-{1}', github.ref, github.event.inputs.post_id != '' && github.event.inputs.post_id || github.run_id) }}
      cancel-in-progress: false

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Notify WP queued
        if: ${{ env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' && github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euo pipefail
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" \
            -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/worker-status" \
            -d "$(jq -n --arg pid "${{ github.event.inputs.post_id }}" \
                       --arg status "queued" \
                       --arg msg "Workflow received and queued" \
                       --arg run "${{ github.run_id }}" \
                       --argjson progress 1 \
                       '{post_id: ($pid|tonumber), status:$status, message:$msg, run_id:$run, progress:$progress}')"

      - name: Install deps
        shell: bash
        run: |
          set -euxo pipefail
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends ffmpeg jq python3 python3-pip ca-certificates gnupg
          python3 -m pip install --upgrade pip
          # core
          python3 -m pip install --no-cache-dir yt-dlp python-twitter tldextract requests
          # enable smart link labeling (KeyBERT + MiniLM)
          python3 -m pip install --no-cache-dir keybert "sentence-transformers>=2.2.0"
          # gcloud for GCS
          echo "deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt cloud-sdk main" | sudo tee /etc/apt/sources.list.d/google-cloud-sdk.list
          curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg
          sudo apt-get update && sudo apt-get install -y google-cloud-sdk
          echo "${{ github.token }}" | docker login ghcr.io -u "${{ github.actor }}" --password-stdin || true

      - name: Validate config and prefixes
        id: cfg
        shell: bash
        run: |
          set -euxo pipefail
          test -n "${GCP_SA_KEY}" || { echo "GCP_SA_KEY missing"; exit 1; }
          test -n "${GCS_BUCKET}" || { echo "GCS_BUCKET missing"; exit 1; }
          mkdir -p "$WORKDIR" "$ARTDIR" "$ARTDIR/logs" ".github/workflows/scripts"
          PFX="$(echo "${{ github.event.inputs.gcs_prefix }}" | sed -E 's#^/*##; s#/*$##')"
          if [ -z "$PFX" ]; then PFX="spaces/$(date +%Y)/$(date +%m)"; fi
          echo "PREFIX=$PFX"                  >> "$GITHUB_ENV"
          echo "BUCKET_PREFIX=${PFX#spaces/}" >> "$GITHUB_ENV"

      - name: Derive Space ID and base
        id: ids
        shell: bash
        env:
          URL: ${{ github.event.inputs.space_url }}
        run: |
          set -euxo pipefail
          SID=""
          if [ -n "$URL" ]; then
            SID="$(echo "$URL" | sed -nE 's#^.*/i/spaces/([^/?#]+).*#\1#p')"
          fi
          [ -z "$SID" ] && SID="unknown"
          BASE="space-$(date +%m-%d-%Y)-${SID}"
          echo "SPACE_ID=${SID}" >> "$GITHUB_ENV"
          echo "BASE=${BASE}"    >> "$GITHUB_ENV"
          echo "space_id=${SID}" >> "$GITHUB_OUTPUT"
          echo "base=${BASE}"    >> "$GITHUB_OUTPUT"

      - name: GCP auth
        if: ${{ github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euxo pipefail
          printf '%s' "${GCP_SA_KEY}" > "${HOME}/gcp-key.json"
          gcloud auth activate-service-account --key-file="${HOME}/gcp-key.json" >/dev/null

      - name: X preflight auth
        id: x_preflight
        if: ${{ github.event.inputs.mode != 'replies_only' }}
        shell: bash
        run: |
          set -euo pipefail
          AUTH="${TWITTER_AUTHORIZATION:-}"
          AT="${TWITTER_AUTH_TOKEN:-}"
          CT="${TWITTER_CSRF_TOKEN:-}"
          if [ -n "$AUTH" ] && ! printf '%s' "$AUTH" | grep -q '^Bearer '; then AUTH=""; fi
          [ -n "${TWITTER_AUTHORIZATION:-}" ] && echo "::add-mask::${TWITTER_AUTHORIZATION}"
          [ -n "$AT" ] && echo "::add-mask::${AT}"
          [ -n "$CT" ] && echo "::add-mask::${CT}"
          OK=0; REASON="no_creds"
          [ -n "$AT" ] && [ -n "$CT" ] && OK=1 && REASON="cookie_ok" || true
          [ -n "$AUTH" ] && OK=1 && REASON="${REASON}_bearer_present" || true
          echo "ok=${OK}"         >> "$GITHUB_OUTPUT"
          echo "reason=${REASON}" >> "$GITHUB_OUTPUT"
          [ -n "$AUTH" ] && echo "TWITTER_AUTHORIZATION=$AUTH" >> "$GITHUB_ENV"

      - name: Run crawler
        id: crawl
        if: ${{ github.event.inputs.mode != 'replies_only' && steps.x_preflight.outputs.ok == '1' }}
        shell: bash
        env:
          SID: ${{ steps.ids.outputs.space_id }}
        run: |
          set -euxo pipefail
          mkdir -p "${ARTDIR}" "${ARTDIR}/logs"
          docker pull ghcr.io/hitomarukonpaku/twspace-crawler:latest || true
          LOG_STD="${ARTDIR}/logs/crawler_${SID}.out.log"
          LOG_ERR="${ARTDIR}/logs/crawler_${SID}.err.log"
          set +e
          timeout 20m docker run --rm \
            -e TWITTER_AUTHORIZATION \
            -e TWITTER_AUTH_TOKEN \
            -e TWITTER_CSRF_TOKEN \
            -v "${ARTDIR}:/app/download" \
            -v "${ARTDIR}/logs:/app/logs" \
            ghcr.io/hitomarukonpaku/twspace-crawler:latest \
            --id "${SID}" --force > >(tee -a "$LOG_STD") 2> >(tee -a "$LOG_ERR" >&2)
          RC=$?
          set -e
          echo "crawler_exit=${RC}"
          AUDIO_FILE="$(find "${ARTDIR}" -type f \( -iname '*.m4a' -o -iname '*.mp3' -o -iname '*.mp4' -o -iname '*.aac' -o -iname '*.webm' -o -iname '*.ogg' -o -iname '*.wav' -o -iname '*.ts' \) -printf '%T@ %p\n' | sort -nr | head -n1 | cut -d' ' -f2- || true)"
          if [ -n "${AUDIO_FILE:-}" ] && [ -f "${AUDIO_FILE}" ]; then
            echo "INPUT_FILE=${AUDIO_FILE}" >> "$GITHUB_ENV"
            echo "audio_file=${AUDIO_FILE}" >> "$GITHUB_OUTPUT"
          fi
          RAW="$(grep -hF 'getAudioSpaceById |' "$LOG_STD" "$LOG_ERR" | tail -n1 || true)"
          if [ -z "$RAW" ]; then
            RAW="$(grep -hF 'getAudioSpaceByRestId |' "$LOG_STD" "$LOG_ERR" | tail -n1 || true)"
          fi
          if [ -n "$RAW" ]; then
            printf '%s\n' "$RAW" | awk -F'\\| ' '{print $NF}' > "${ARTDIR}/_as_line.json" || true
          fi
          [ -s "${ARTDIR}/_as_line.json" ] && echo "as_line=${ARTDIR}/_as_line.json" >> "$GITHUB_OUTPUT" || true
          CC_JSONL="$(find "${ARTDIR}" -type f \( -iname '*cc.jsonl' -o -iname '*caption*.jsonl' -o -iname '*captions*.jsonl' \) -print | head -n1 || true)"
          if [ -n "${CC_JSONL:-}" ]; then
            echo "CRAWLER_CC=${CC_JSONL}" >> "$GITHUB_ENV"
          fi

      - name: Fallback download via yt dlp
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && (steps.crawl.outputs.audio_file == '' || steps.crawl.outcome != 'success') && github.event.inputs.existing_mp3_url == '' && github.event.inputs.space_url != '' }}
        shell: bash
        working-directory: ${{ env.WORKDIR }}
        env:
          URL: ${{ github.event.inputs.space_url }}
        run: |
          set -euxo pipefail
          yt-dlp -o "%(title)s.%(ext)s" -f "bestaudio/best" "$URL"
          IN="$(ls -S | head -n1 || true)"
          test -f "$IN" || { echo "No file downloaded"; exit 1; }
          echo "INPUT_FILE=$PWD/$IN" >> "$GITHUB_ENV"

      - name: Use provided MP3 for transcript only
        if: ${{ github.event.inputs.mode == 'transcript_only' && github.event.inputs.existing_mp3_url != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          curl -L "${{ github.event.inputs.existing_mp3_url }}" -o "${ARTDIR}/${BASE}.mp3"
          echo "INPUT_FILE=${ARTDIR}/${BASE}.mp3" >> "$GITHUB_ENV"

      - name: Detect lead silence seconds
        id: detect
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.INPUT_FILE != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          LOG="${WORKDIR}/silence.log"
          ffmpeg -hide_banner -i "$INPUT_FILE" -af "silencedetect=noise=-45dB:d=1" -f null - 2> "$LOG" || true
          LEAD="$(awk '/silence_end/ {print $5; exit}' "$LOG" || true)"
          case "$LEAD" in ''|*[^0-9.]* ) LEAD="0.0" ;; esac
          echo "TRIM_LEAD=${LEAD}" >> "$GITHUB_ENV"
          echo "lead=${LEAD}"       >> "$GITHUB_OUTPUT"

      - name: Trim head and tail (RF64-safe)
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.INPUT_FILE != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          TRIM_WAV="${WORKDIR}/trim_${{ github.run_id }}.wav"
          ffmpeg -hide_banner -y -i "$INPUT_FILE" \
            -af "silenceremove=start_periods=1:start_silence=1:start_threshold=-45dB:detection=peak,areverse,silenceremove=start_periods=1:start_silence=1:start_threshold=-45dB:detection=peak,areverse" \
            -rf64 always -c:a pcm_s16le "$TRIM_WAV"
          echo "AUDIO_IN=${TRIM_WAV}" >> "$GITHUB_ENV"

      - name: Probe audio format
        id: probe
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.AUDIO_IN != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          J="$(ffprobe -v error -select_streams a:0 -show_entries stream=channels,sample_rate -of json "$AUDIO_IN")"
          CH=$(echo "$J" | jq -r '.streams[0].channels // 1')
          SR=$(echo "$J" | jq -r '.streams[0].sample_rate // "48000"')
          echo "SRC_CH=${CH}" >> "$GITHUB_ENV"
          echo "SRC_SR=${SR}" >> "$GITHUB_ENV"

      - name: Encode MP3 with selected profile
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.AUDIO_IN != '' }}
        shell: bash
        env:
          PROF: ${{ github.event.inputs.audio_profile != '' && github.event.inputs.audio_profile || 'radio' }}
        run: |
          set -euxo pipefail
          OUT="${ARTDIR}/${BASE}.mp3"
          CH="${SRC_CH:-1}"
          SR="${SRC_SR:-48000}"
          if [ "${PROF}" = "transparent" ]; then
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -map a:0 -c:a libmp3lame -q:a 0 -ar "$SR" -ac "$CH" "$OUT"
          elif [ "${PROF}" = "radio" ]; then
            PRE="highpass=f=60,lowpass=f=14000,afftdn=nr=4:nf=-28,deesser=i=0.12,acompressor=threshold=-18dB:ratio=2:attack=12:release=220:makeup=2"
            PASS1_JSON="${WORKDIR}/loudnorm1.json"
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:print_format=json" -f null - 2>"${WORKDIR}/pass1.log" || true
            awk '/^{/{f=1} f{print} /}/{f=0}' "${WORKDIR}/pass1.log" > "$PASS1_JSON" || true
            if jq -e . "$PASS1_JSON" >/dev/null 2>&1; then
              I=$(jq -r '.input_i // "-16"'  "$PASS1_JSON")
              TP=$(jq -r '.input_tp // "-1.5"' "$PASS1_JSON")
              LRA=$(jq -r '.input_lra // "11"' "$PASS1_JSON")
              TH=$(jq -r '.input_thresh // "-26"' "$PASS1_JSON")
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:measured_I=$I:measured_TP=$TP:measured_LRA=$LRA:measured_thresh=$TH:linear=true" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            else
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            fi
          else
            PRE="highpass=f=70,lowpass=f=11500,afftdn=nr=8:nf=-25,deesser=i=0.2,acompressor=threshold=-18dB:ratio=2.8:attack=8:release=200:makeup=3"
            if [ "${{ github.event.inputs.aggressive_denoise }}" = "true" ]; then
              M="${WORKDIR}/rnnoise.rnnn"
              curl -fsSL -o "$M" https://raw.githubusercontent.com/GregorR/rnnoise-models/master/heavyrnnoise.rnnn || true
              if [ -s "$M" ]; then PRE="arnndn=m=${M},${PRE}"; fi
            fi
            PASS1_JSON="${WORKDIR}/loudnorm1.json"
            ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:print_format=json" -f null - 2>"${WORKDIR}/pass1.log" || true
            awk '/^{/{f=1} f{print} /}/{f=0}' "${WORKDIR}/pass1.log" > "$PASS1_JSON" || true
            if jq -e . "$PASS1_JSON" >/dev/null 2>&1; then
              I=$(jq -r '.input_i // "-16"'  "$PASS1_JSON")
              TP=$(jq -r '.input_tp // "-1.5"' "$PASS1_JSON")
              LRA=$(jq -r '.input_lra // "11"' "$PASS1_JSON")
              TH=$(jq -r '.input_thresh // "-26"' "$PASS1_JSON")
              ffmpeg -hide_banner -y -i "$AUDIO_IN" \
                -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11:measured_I=$I:measured_TP=$TP:measured_LRA=$LRA:measured_thresh=$TH:linear=true" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            else
              ffmpeg -hide_banner -y -i "$AUDIO_IN" -af "${PRE},loudnorm=I=-16:TP=-1.5:LRA=11" \
                -c:a libmp3lame -q:a 2 -ar "$SR" -ac "$CH" "$OUT"
            fi
          fi
          echo "MP3_PATH=${OUT}" >> "$GITHUB_ENV"

      - name: Upload MP3 to GCS
        id: upload_mp3
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.MP3_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          DEST="gs://${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.mp3"
          RAW="https://storage.googleapis.com/${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.mp3"
          PROXY="https://media.chbmp.org/${PREFIX}/${BASE}.mp3"
          gsutil -m cp "${MP3_PATH}" "$DEST"
          if [ "${{ github.event.inputs.make_public }}" = "true" ]; then
            (gsutil acl ch -u AllUsers:R "$DEST" || gsutil iam ch allUsers:objectViewer "gs://${GCS_BUCKET}") || true
          fi
          echo "audio_raw=${RAW}"     >> "$GITHUB_OUTPUT"
          echo "audio_proxy=${PROXY}" >> "$GITHUB_OUTPUT"

      - name: Write helper scripts (gen_vtt, polish, replies)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p ".github/workflows/scripts"

          # ---------------- gen_vtt.py ----------------
          cat > ".github/workflows/scripts/gen_vtt.py" << 'PY'
          #!/usr/bin/env python3
          import json, os, re, html, sys
          from datetime import datetime, timezone

          artdir = os.environ.get("ARTDIR") or ""
          base   = os.environ.get("BASE") or ""
          src    = os.environ.get("CC_JSONL") or ""
          shift  = float(os.environ.get("SHIFT_SECS") or "0")

          if not (artdir and base and src and os.path.isfile(src)):
              os.makedirs(artdir, exist_ok=True)
              open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8").write("WEBVTT\n\n")
              open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8").write("")
              # ensure a start marker exists (used by replies pulse)
              open(os.path.join(artdir,f"{base}.start.txt"),"w",encoding="utf-8").write("")
              sys.exit(0)

          EMOJI_RE = re.compile("[" +
              "\U0001F1E6-\U0001F1FF" "\U0001F300-\U0001F5FF" "\U0001F600-\U0001F64F" "\U0001F680-\U0001F6FF" +
              "\U0001F700-\U0001F77F" "\U0001F780-\U0001F7FF" "\U0001F800-\U0001F8FF" "\U0001F900-\U0001F9FF" +
              "\U0001FA00-\U0001FAFF" "\u2600-\u26FF" "\u2700-\u27BF" + "]+", re.UNICODE)
          ONLY_PUNCT_SPACE = re.compile(r"^[\s\.,;:!?\-–—'\"“”‘’•·]+$")

          def is_emoji_only(s: str) -> bool:
              if not s or not s.strip(): return False
              t = ONLY_PUNCT_SPACE.sub("", s)
              t = EMOJI_RE.sub("", t)
              return len(t.strip()) == 0

          def parse_time_iso(s):
              if not s: return None
              s=s.strip()
              try:
                  if s.endswith('Z'): s=s[:-1]+'+00:00'
                  if re.search(r'[+-]\d{4}$', s):
                      s=s[:-5]+s[-5:-3]+':'+s[-3:]
                  dt=datetime.fromisoformat(s)
                  if dt.tzinfo is None: dt=dt.replace(tzinfo=timezone.utc)
                  return dt.timestamp()
              except: return None

          def to_float_seconds(x):
              try:
                  if x is None: return None
                  f=float(x)
                  return f/1000.0 if f>4_000_000 else f
              except: return None

          def first(*vals):
              for v in vals:
                  if v not in (None,""): return v
              return None

          def esc(s): return (s or "").replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

          REL_KEYS  = ("offset","startSec","startMs","start")
          ABS_KEYS  = ("timestamp","programDateTime","ts")

          raw_items=[]
          reactions=[]
          abs_candidates=[]

          with open(src,'r',encoding='utf-8',errors='ignore') as f:
              for line in f:
                  line=line.strip()
                  if not line: continue
                  try: obj=json.loads(line)
                  except: continue

                  layer=None; sender={}
                  if isinstance(obj,dict) and isinstance(obj.get("payload"),str):
                      try:
                          pl=json.loads(obj["payload"])
                          if isinstance(pl,dict) and isinstance(pl.get("body"),str):
                              try:
                                  layer=json.loads(pl["body"]); sender=pl.get("sender") or {}
                              except: layer=None
                      except: pass
                  else:
                      layer=obj

                  def push_item(rel_ts, abs_ts, txt, disp, uname, avatar):
                      if not txt: return
                      if is_emoji_only(txt):
                          if abs_ts is not None or rel_ts is not None:
                              reactions.append({
                                  "t_abs": abs_ts, "t_rel": rel_ts, "emoji": txt,
                                  "name": (disp or uname or "") or "", "handle": (uname or "").lstrip("@"),
                                  "avatar": avatar or ""
                              })
                          return
                      raw_items.append({
                          "rel": rel_ts, "abs": abs_ts, "text": txt,
                          "name": (disp or uname or "Speaker"),
                          "username": (uname or "").lstrip("@"),
                          "avatar": avatar or ""
                      })
                      if abs_ts is not None: abs_candidates.append(abs_ts)

                  def pick_rel_abs(d):
                      rel=None
                      for k in REL_KEYS:
                          if k in d and d[k] not in (None,""):
                              rel = to_float_seconds(d[k]); 
                              if rel is not None: break
                      abs_ts=None
                      for k in ABS_KEYS:
                          if k in d and d[k] not in (None,""):
                              abs_ts = parse_time_iso(d[k]) if k=="programDateTime" else to_float_seconds(d[k])
                              if abs_ts is not None: break
                      return rel, abs_ts

                  candidates=[]
                  if isinstance(layer,dict) and layer:
                      d=layer
                      txt = first(d.get("body"), d.get("text"), d.get("caption"))
                      disp=first(d.get("displayName"), d.get("speaker_name"), d.get("speakerName"), (sender or {}).get("display_name"))
                      uname=first(d.get("username"), d.get("user_id"), (sender or {}).get("screen_name"))
                      avat=first((sender or {}).get("profile_image_url_https"), (sender or {}).get("profile_image_url"))
                      rel,abs_ts = pick_rel_abs(d)
                      if txt: candidates.append((rel,abs_ts,txt,disp,uname,avat))

                  if isinstance(obj,dict):
                      txt = first(obj.get("text"), obj.get("caption"), obj.get("payloadText"))
                      disp=first(obj.get("displayName"), obj.get("speaker"), obj.get("user"), obj.get("name"))
                      uname=first(obj.get("username"), obj.get("handle"), obj.get("screen_name"))
                      avat=first(obj.get("profile_image_url_https"), obj.get("profile_image_url"))
                      rel,abs_ts = pick_rel_abs(obj)
                      if txt: candidates.append((rel,abs_ts,txt,disp,uname,avat))

                  for (rel,abs_ts,txt,disp,uname,avat) in candidates:
                      if not re.search(r"[A-Za-z0-9]", txt) and not is_emoji_only(txt): continue
                      push_item(rel,abs_ts,txt,disp,uname,avat)

          if not raw_items:
              os.makedirs(artdir, exist_ok=True)
              open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8").write("WEBVTT\n\n")
              open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8").write("")
              open(os.path.join(artdir,f"{base}_reactions.json"),"w",encoding="utf-8").write("[]")
              open(os.path.join(artdir,f"{base}.start.txt"),"w",encoding="utf-8").write("")
              sys.exit(0)

          abs0 = min(abs_candidates) if abs_candidates else None
          norm=[]
          for it in raw_items:
              if it["rel"] is not None: t = it["rel"]
              elif it["abs"] is not None and abs0 is not None: t = it["abs"] - abs0
              else: t = 0.0
              t = max(0.0, t - shift)
              norm.append({ **it, "t": float(t) })

          norm.sort(key=lambda x: x["t"])
          EPS=0.0005; last=-1e9
          for u in norm:
              if u["t"] <= last: u["t"] = last + EPS
              last = u["t"]

          def fmt_ts(t):
              if t<0: t=0.0
              h=int(t//3600); m=int((t%3600)//60); s=t%60
              return f"{h:02d}:{m:02d}:{s:06.3f}"

          MERGE_GAP=3.0; groups=[]; cur=None
          for u in norm:
              if cur and u["username"]==cur["username"] and u["name"]==cur["name"] and (u["t"]-cur["end"])<=MERGE_GAP:
                  sep = "" if re.search(r'[.!?]"?$', cur["text"]) else " "
                  cur["text"]=(cur["text"]+sep+u["text"]).strip()
                  cur["end"]=max(cur["end"], u["t"]+0.8)
              else:
                  cur={"name":u["name"],"username":u["username"],"avatar":u["avatar"],
                       "start":u["t"],"end":u["t"]+0.8,"text":u["text"]}
                  groups.append(cur)

          MIN_DUR=0.80; MAX_DUR=10.0; GUARD=0.020
          for i,g in enumerate(groups):
              if i+1<len(groups):
                  nxt=groups[i+1]["start"]
                  dur=max(MIN_DUR, min(MAX_DUR, (nxt-g["start"])-GUARD))
                  g["end"]=g["start"]+dur
              else:
                  words=max(1, len(g["text"].split()))
                  dur=max(MIN_DUR, min(MAX_DUR, 0.33*words+0.7))
                  g["end"]=g["start"]+dur

          os.makedirs(artdir, exist_ok=True)
          with open(os.path.join(artdir,f"{base}.vtt"),"w",encoding="utf-8") as vf:
              vf.write("WEBVTT\n\n")
              for i,g in enumerate(groups,1):
                  vf.write(f"{i}\n{fmt_ts(g['start'])} --> {fmt_ts(g['end'])}\n")
                  vf.write(f"<v {esc(g['name'])}> {esc(g['text'])}\n\n")

          css='''
          <style>
          .ss3k-transcript{font:15px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;
            max-height:70vh; overflow-y:auto; scroll-behavior:smooth; border:1px solid #e5e7eb; border-radius:12px; padding:6px}
          .ss3k-seg{display:flex;gap:10px;padding:8px 10px;border-radius:10px;margin:6px 0}
          .ss3k-seg.active{background:#eef6ff;outline:1px solid #bfdbfe}
          .ss3k-avatar{width:26px;height:26px;border-radius:50%;flex:0 0 26px;margin-top:3px;background:#e5e7eb}
          .ss3k-meta{font-size:12px;color:#64748b;margin-bottom:2px}
          .ss3k-name a{color:#0f172a;text-decoration:none}
          .ss3k-text{white-space:pre-wrap;word-break:break-word;cursor:pointer}
          </style>
          '''
          js='''
          <script>
          (function(){
            function time(s){return parseFloat(s||'0')||0}
            function within(t,seg){return t>=time(seg.dataset.start) && t<time(seg.dataset.end)}
            function bind(){
              var audio=document.getElementById('ss3k-audio')||document.querySelector('audio[data-ss3k-player]');
              var cont=document.querySelector('.ss3k-transcript'); if(!audio||!cont) return;
              var segs=[].slice.call(cont.querySelectorAll('.ss3k-seg'));
              function tick(){
                var t=audio.currentTime||0, found=null;
                for(var i=0;i<segs.length;i++){ if(within(t,segs[i])){found=segs[i];break;} }
                segs.forEach(function(s){ s.classList.toggle('active', s===found); });
                if(found){
                  var top = found.offsetTop - cont.offsetTop;
                  if (Math.abs(cont.scrollTop - top) > 6) cont.scrollTop = top;
                }
              }
              audio.addEventListener('timeupdate', tick);
              audio.addEventListener('seeked', tick);
              segs.forEach(function(s){
                s.addEventListener('click', function(){
                  audio.currentTime = time(s.dataset.start)+0.05; audio.play().catch(function(){});
                });
              });
              tick();
            }
            if(document.readyState!=="loading") bind(); else document.addEventListener('DOMContentLoaded', bind);
          })();
          </script>
          '''
          with open(os.path.join(artdir,f"{base}_transcript.html"),"w",encoding="utf-8") as tf:
              def fmt_ts(t):
                  if t<0: t=0.0
                  h=int(t//3600); m=int((t%3600)//60); s=t%60
                  return f"{h:02d}:{m:02d}:{s:06.3f}"
              tf.write(css); tf.write('<div class="ss3k-transcript">\\n')
              for i,g in enumerate(groups,1):
                  uname=(g["username"] or "").lstrip("@")
                  prof=f"https://x.com/{html.escape(uname, True)}" if uname else ""
                  avatar=g.get("avatar") or (f"https://unavatar.io/x/{html.escape(uname, True)}" if uname else "")
                  if avatar and prof:
                      avtag=f'<a href="{prof}" target="_blank" rel="noopener"><img class="ss3k-avatar" src="{html.escape(avatar, True)}" alt=""></a>'
                  elif avatar:
                      avtag=f'<img class="ss3k-avatar" src="{html.escape(avatar, True)}" alt="">'
                  else:
                      avtag='<div class="ss3k-avatar" aria-hidden="true"></div>'
                  name_html = f'<span class="ss3k-name"><strong>{html.escape(g["name"], True)}</strong></span>'
                  if prof:
                      name_html = f'<span class="ss3k-name"><a href="{prof}" target="_blank" rel="noopener"><strong>{html.escape(g["name"], True)}</strong></a></span>'
                  tf.write(f'<div class="ss3k-seg" id="seg-{i:04d}" data-start="{g["start"]:.3f}" data-end="{g["end"]:.3f}"')
                  if uname: tf.write(f' data-handle="@{html.escape(uname, True)}"')
                  tf.write('>')
                  tf.write(avtag)
                  tf.write('<div class="ss3k-body">')
                  tf.write(f'<div class="ss3k-meta">{name_html} · <time>{fmt_ts(g["start"])}</time>–<time>{fmt_ts(g["end"])}</time></div>')
                  tf.write(f'<div class="ss3k-text">{html.escape(g["text"], True)}</div>')
                  tf.write('</div></div>\\n')
              tf.write('</div>\\n'); tf.write(js)

          # reactions export
          rx=[]
          if reactions:
              abs0 = min(abs_candidates) if abs_candidates else None
              for r in reactions:
                  if r["t_rel"] is not None:
                      t=r["t_rel"]
                  elif r["t_abs"] is not None and abs0 is not None:
                      t=r["t_abs"]-abs0
                  else:
                      continue
                  t=max(0.0, t - shift)
                  rx.append({"t": round(t,3), "emoji": r["emoji"], "name": r["name"], "handle": r["handle"], "avatar": r["avatar"]})
          with open(os.path.join(artdir,f"{base}_reactions.json"),"w",encoding="utf-8") as rf:
              json.dump(rx, rf, ensure_ascii=False)

          # write absolute start time for replies pulse
          start_iso=""
          if abs_candidates:
              start_iso = datetime.fromtimestamp(min(abs_candidates), timezone.utc).isoformat(timespec='seconds').replace('+00:00','Z')
          with open(os.path.join(artdir,f"{base}.start.txt"),"w",encoding="utf-8") as sf:
              sf.write((start_iso or "") + "\\n")
          PY
          chmod +x ".github/workflows/scripts/gen_vtt.py"

          # ---------------- polish_transcript.py ----------------
          cat > ".github/workflows/scripts/polish_transcript.py" << 'PY'
          #!/usr/bin/env python3
          import os, re, html, json, time
          ARTDIR = os.environ.get("ARTDIR",".")
          BASE   = os.environ.get("BASE","space")
          INP    = os.path.join(ARTDIR, f"{BASE}_transcript.html")
          OUT    = os.path.join(ARTDIR, f"{BASE}_transcript_polished.html")
          REPORT = os.path.join(ARTDIR, f"{BASE}_transcript_polish_report.json")
          if not os.path.exists(INP) or os.path.getsize(INP) == 0: raise SystemExit(0)
          t0=time.time(); raw=open(INP,"r",encoding="utf-8",errors="ignore").read()
          TEXT = re.compile(r'(<(?:div|span)\s+class="ss3k-text"[^>]*>)(.*?)(</(?:div|span)>)', re.S|re.I)
          URL_RE = re.compile(r"https?://[^\s<>\"]+")
          EMOJI_RE = re.compile("[" + "\U0001F1E6-\U0001F1FF" "\U0001F300-\U0001F5FF" "\U0001F600-\U0001F64F" "\U0001F680-\U0001F6FF" + "\U0001F700-\U0001F77F" "\U0001F780-\U0001F7FF" "\U0001F800-\U0001F8FF" "\U0001F900-\U0001F9FF" + "\U0001FA00-\U0001FAFF" "\u2600-\u26FF" "\u2700-\u27BF" + "]+", re.UNICODE)
          ONLY_PUNCT_SPACE = re.compile(r"^[\s\.,;:!?\-–—'\"“”‘’•·]+$")
          FILLERS_RE = re.compile(r"\b(uh+|um+|er+|ah+|mm+h*|hmm+|eh+|you\s+know|i\s+mean|sort\s+of|kind\s+of)\b", re.I)
          REPEAT_RE  = re.compile(r"\b([A-Za-z]{2,})\b(?:\s+\1\b){1,4}", re.I)
          def is_emoji_only(s:str)->bool:
              if not s.strip(): return False
              t = ONLY_PUNCT_SPACE.sub("", s); t = EMOJI_RE.sub("", t); return len(t.strip())==0
          def sentence_case(s:str)->str:
              s=re.sub(r"\bi\b","I",s)
              def cap(m): return (m.group(1) or "") + m.group(2).upper()
              return re.sub(r"(^|\.\s+|\?\s+|!\s+)([a-z])", cap, s)
          def ensure_end_punct(s:str)->str:
              t=s.rstrip()
              if not t: return s
              if t[-1] in ".!?\":)””’'»]>": return s
              if len(re.findall(r"\b\w+\b", t)) >= 6: return t+"."
              return s
          def base_clean(txt:str)->str:
              if not txt.strip(): return txt
              if is_emoji_only(txt): return ""
              txt = EMOJI_RE.sub("", txt)
              txt = FILLERS_RE.sub("", txt)
              txt = REPEAT_RE.sub(lambda m: m.group(1), txt)
              txt = re.sub(r"\s{2,}", " ", txt).strip()
              txt = re.sub(r"\s+([,.;:!?])", r"\1", txt)
              txt = re.sub(r"([,;:])([^\s])", r"\1 \2", txt)
              txt = sentence_case(txt); txt = ensure_end_punct(txt)
              return txt
          spans=[]; TEXT.sub(lambda m: spans.append(m.group(2)) or m.group(0), raw)
          cleaned=[base_clean(t) for t in spans]
          it=iter(cleaned)
          def repl(m):
              return f"{m.group(1)}{next(it, m.group(2))}{m.group(3)}"
          out=TEXT.sub(repl, raw)
          open(OUT,"w",encoding="utf-8").write(out)
          open(REPORT,"w",encoding="utf-8").write(json.dumps({
            "nodes": len(spans),
            "changed": sum(1 for a,b in zip(spans,cleaned) if a!=b),
            "duration_sec": round(time.time()-t0,3)
          }, ensure_ascii=False, indent=2))
          PY
          chmod +x ".github/workflows/scripts/polish_transcript.py"

          # ---------------- replies.py (flatten self-replies + pulse + smart labels via KeyBERT/titles/context) ----------------
          cat > ".github/workflows/scripts/replies.py" << 'PY'
          #!/usr/bin/env python3
          import os, re, json, html, time
          from collections import defaultdict
          from datetime import datetime, timezone

          # Optional deps (gracefully degrade if missing)
          try:
              import tldextract
          except Exception:
              tldextract = None

          ARTDIR = os.environ.get("ARTDIR",".")
          BASE   = os.environ.get("BASE","space")
          PURPLE = (os.environ.get("PURPLE_TWEET_URL","") or "").strip()

          REPLIES_OUT = os.path.join(ARTDIR, f"{BASE}_replies.html")
          LINKS_OUT   = os.path.join(ARTDIR, f"{BASE}_links.html")
          START_PATH  = os.path.join(ARTDIR, f"{BASE}.start.txt")

          # --- Link labeling config (can be overridden via env) ---
          LINK_LABEL_AI         = (os.environ.get("LINK_LABEL_AI","keybert") or "keybert").lower()  # keybert|off
          LINK_LABEL_MODEL      = os.environ.get("LINK_LABEL_MODEL","sentence-transformers/all-MiniLM-L6-v2")
          FETCH_TITLES          = (os.environ.get("LINK_LABEL_FETCH_TITLES","true") or "true").lower() in ("1","true","yes","on")
          FETCH_TITLES_LIMIT    = int(os.environ.get("LINK_LABEL_FETCH_LIMIT","15") or "15")
          FETCH_TIMEOUT_SEC     = int(os.environ.get("LINK_LABEL_TIMEOUT_SEC","4") or "4")
          KEYBERT_TOPN          = int(os.environ.get("KEYBERT_TOPN","8") or "8")
          KEYBERT_NGRAM_MIN     = int(os.environ.get("KEYBERT_NGRAM_MIN","1") or "1")
          KEYBERT_NGRAM_MAX     = int(os.environ.get("KEYBERT_NGRAM_MAX","3") or "3")
          KEYBERT_USE_MMR       = (os.environ.get("KEYBERT_USE_MMR","true") or "true").lower() in ("1","true","yes","on")
          KEYBERT_DIVERSITY     = float(os.environ.get("KEYBERT_DIVERSITY","0.6") or "0.6")

          def esc(x:str)->str: return html.escape(x or "")

          def write_empty():
              open(REPLIES_OUT,"w",encoding="utf-8").write("")
              open(LINKS_OUT,"w",encoding="utf-8").write("")

          def parse_created_at(s):
              try:  return datetime.strptime(s, "%a %b %d %H:%M:%S %z %Y").timestamp()
              except Exception: return 0.0

          def fmt_utc(ts):
              try:  return datetime.fromtimestamp(ts, tz=timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
              except Exception: return ""

          def load_start_epoch(p):
              try:
                  if not os.path.isfile(p): return None
                  iso=open(p,"r",encoding="utf-8",errors="ignore").read().strip()
                  if not iso: return None
                  if iso.endswith("Z"): iso=iso[:-1]+"+00:00"
                  m=re.search(r"[+-]\d{4}$", iso)
                  if m: iso=iso[:-5]+iso[-5:-3]+":"+iso[-3:]
                  dt=datetime.fromisoformat(iso)
                  if dt.tzinfo is None: dt=dt.replace(tzinfo=timezone.utc)
                  return dt.timestamp()
              except Exception:
                  return None

          # ---------- HTTP helpers ----------
          def http_json(url, method="GET", headers=None, data=None, timeout=30):
              import urllib.request
              req=urllib.request.Request(url=url, method=method)
              if headers:
                  for k,v in headers.items(): req.add_header(k,v)
              body = data.encode("utf-8") if isinstance(data,str) else data
              try:
                  with urllib.request.urlopen(req, data=body, timeout=timeout) as r:
                      txt=r.read().decode("utf-8","ignore")
                      return json.loads(txt)
              except Exception:
                  return None

          def http_get(url, timeout=FETCH_TIMEOUT_SEC, max_bytes=32768):
              import urllib.request
              try:
                  req = urllib.request.Request(url, headers={"User-Agent":"Mozilla/5.0"})
                  with urllib.request.urlopen(req, timeout=timeout) as r:
                      data=r.read(max_bytes)
                      charset="utf-8"
                      ct=r.headers.get("Content-Type","")
                      m=re.search(r"charset=([\\w\\-]+)", ct or "", re.I)
                      if m: charset=m.group(1)
                      try: return data.decode(charset,"ignore")
                      except Exception: return data.decode("utf-8","ignore")
              except Exception:
                  return None

          def extract_title(html_text):
              if not html_text: return None
              m=re.search(r"<title[^>]*>(.*?)</title>", html_text, re.I|re.S)
              if not m: return None
              t=re.sub(r"\\s+"," ", html.unescape(m.group(1))).strip()
              return t or None

          def get_guest_token(bearer):
              if not bearer: return None
              headers={"authorization":bearer,"content-type":"application/json","user-agent":"Mozilla/5.0"}
              d=http_json("https://api.twitter.com/1.1/guest/activate.json", method="POST", headers=headers, data="{}")
              return (d or {}).get("guest_token")

          def find_bottom_cursor(obj):
              if isinstance(obj, dict):
                  if obj.get("cursorType")=="Bottom" and "value" in obj: return obj["value"]
                  for v in obj.values():
                      c=find_bottom_cursor(v)
                      if c: return c
              elif isinstance(obj, list):
                  for it in obj:
                      c=find_bottom_cursor(it)
                      if c: return c
              return None

          def fetch_conversation_adaptive(root_id_str, screen_name_hint=None, max_pages=60, page_sleep=0.6):
              bearer=(os.environ.get("TWITTER_AUTHORIZATION","") or "").strip()
              at=(os.environ.get("TWITTER_AUTH_TOKEN","") or "").strip()
              ct0=(os.environ.get("TWITTER_CSRF_TOKEN","") or "").strip()
              if not (at and ct0) and not bearer: return None, None
              headers={"user-agent":"Mozilla/5.0","accept":"application/json, text/plain, */*","pragma":"no-cache","cache-control":"no-cache",
                       "referer": f"https://x.com/{screen_name_hint}/status/{root_id_str}" if screen_name_hint else "https://x.com/"}
              if at and ct0:
                  if bearer and not bearer.startswith("Bearer "): bearer=""
                  headers.update({"authorization": bearer or "Bearer","x-csrf-token": ct0,"cookie": f"auth_token={at}; ct0={ct0}"})
              else:
                  if not bearer: return None, None
                  gt=get_guest_token(bearer)
                  if not gt: return None, None
                  headers.update({"authorization": bearer,"x-guest-token": gt})

              base="https://twitter.com/i/api/2/search/adaptive.json"
              q=f"conversation_id:{root_id_str}"
              tweets, users, cursor, pages = {}, {}, None, 0
              while pages < max_pages:
                  params={"q": q, "count": 100, "tweet_search_mode":"live","query_source":"typed_query","tweet_mode":"extended",
                          "pc":"ContextualServices","spelling_corrections":"1","include_quote_count":"true","include_reply_count":"true",
                          "ext":"mediaStats,highlightedLabel,hashtags,antispam_media_platform,voiceInfo,superFollowMetadata,unmentionInfo,editControl,emoji_reaction"}
                  if cursor: params["cursor"]=cursor
                  url = base + "?" + "&".join(f"{k}={json.dumps(v)[1:-1]}" for k,v in params.items())
                  data=http_json(url, headers=headers)
                  if not data: break
                  g=(data.get("globalObjects") or {})
                  tw=g.get("tweets") or {}
                  us=g.get("users") or {}
                  if tw: tweets.update(tw)
                  if us: users.update(us)
                  nxt=find_bottom_cursor(data.get("timeline") or data)
                  pages+=1
                  if not nxt or nxt==cursor: break
                  cursor=nxt
                  time.sleep(page_sleep)
              return tweets or None, users or None

          # ---------- Optional KeyBERT ----------
          _keybert = None
          def kb_label(corpus_text):
              global _keybert
              if LINK_LABEL_AI != "keybert":
                  return None
              try:
                  if _keybert is None:
                      from keybert import KeyBERT
                      from sentence_transformers import SentenceTransformer
                      st_model = SentenceTransformer(LINK_LABEL_MODEL)
                      _keybert = KeyBERT(model=st_model)
                  kws = _keybert.extract_keywords(
                      corpus_text,
                      keyphrase_ngram_range=(KEYBERT_NGRAM_MIN, KEYBERT_NGRAM_MAX),
                      stop_words="english",
                      use_mmr=KEYBERT_USE_MMR,
                      diversity=KEYBERT_DIVERSITY,
                      top_n=KEYBERT_TOPN
                  )
                  if not kws: return None
                  best = sorted(kws, key=lambda x: x[1], reverse=True)[0][0]
                  return compress_to_2_3_words(best)
              except Exception:
                  return None

          STOPWORDS = {
              "the","and","for","to","of","in","on","with","a","an","is","are","be","this","that","from","by","at","as","into",
              "your","our","their","about","or","it","its","you","we","they","them","his","her","him","than","then","over","out"
          }
          def smart_words(text): return [w for w in re.findall(r"[A-Za-z0-9]+", text or "") if w]
          def compress_to_2_3_words(text):
              if not text: return "Link"
              seg = re.split(r"\\s+[-–—|:]\\s+", text.strip(), maxsplit=1)[0]
              tokens = smart_words(seg)
              kept=[]
              for w in tokens:
                  if len(kept)>=3: break
                  lw=w.lower()
                  if lw in STOPWORDS and len(tokens)>3: continue
                  kept.append(w if w.isupper() else w.capitalize())
              if not kept:
                  kept = [tokens[0].capitalize()] if tokens else ["Link"]
              return " ".join(kept[:3])

          def slug_to_words(u):
              m = re.match(r"https?://[^/]+/(.+)", u or "")
              if not m: return []
              slug = m.group(1).strip("/").split("/")[-1]
              slug = re.sub(r"\\.[A-Za-z0-9]{1,6}$","",slug)
              slug = slug.replace("_","-")
              parts=[p for p in slug.split("-") if p]
              out=[]
              for p in parts:
                  out.extend(re.findall(r"[A-Z]+(?![a-z])|[A-Z]?[a-z]+|\\d+", p))
              return out

          def derive_from_context(text):
              if not text: return None
              tags = re.findall(r"#([A-Za-z0-9_]+)", text)
              if tags:
                  tag = re.sub(r"([A-Za-z])(\\d)", r"\\1 \\2", tags[0])
                  tag = re.sub(r"(\\d)([A-Za-z])", r"\\1 \\2", tag)
                  return compress_to_2_3_words(tag)
              up = re.findall(r"\\b([A-Z][A-Z0-9]+(?:\\s+[A-Z][A-Z0-9]+){1,3})\\b", text)
              if up: return compress_to_2_3_words(up[0])
              toks = [t for t in re.findall(r"[A-Za-z][A-Za-z0-9']+", text) if t.lower() not in STOPWORDS]
              toks.sort(key=lambda w:(-w[0].isupper(), -len(w)))
              if toks: return compress_to_2_3_words(" ".join(toks[:3]))
              return None

          def label_from_domain(domain):
              if not domain: return "Link"
              d=domain.lower()
              if "youtube.com" in d or "youtu.be" in d: return "YouTube Video"
              if "substack.com" in d: return "Substack Post"
              if "x.com" in d or "twitter.com" in d: return "Tweet"
              if "pubmed." in d: return "PubMed Study"
              if "who.int" in d: return "WHO Page"
              if "github.com" in d: return "GitHub Repo"
              if "medium.com" in d: return "Medium Post"
              if d.endswith(".gov"): return "Gov Page"
              return (d.split(".")[-2].capitalize()+" Page") if "." in d else d.capitalize()

          # ---------- Link index with labeling ----------
          class LinkIndex:
              def __init__(self):
                  self.data = defaultdict(dict)   # domain -> url -> info
                  self.urls_seen=set()
                  self.all_urls=[]

              def _domain_of(self, url):
                  if tldextract:
                      try:
                          ext=tldextract.extract(url)
                          return ".".join([p for p in (ext.domain, ext.suffix) if p]).lower()
                      except Exception:
                          pass
                  m=re.match(r"https?://([^/]+)/?", url or "")
                  return (m.group(1).lower() if m else (url or ""))

              def add(self, url, context=None):
                  if not url: return
                  dom=self._domain_of(url)
                  if url not in self.data[dom]:
                      self.data[dom][url]={"url":url, "contexts":set(), "label":None, "domain":dom, "title":None}
                      if url not in self.urls_seen:
                          self.urls_seen.add(url); self.all_urls.append(url)
                  if context:
                      ctx = re.sub(r"\\s+"," ", context).strip()
                      if ctx: self.data[dom][url]["contexts"].add(ctx[:240])

              def finalize_labels(self):
                  # 1) fetch titles for a subset
                  fetched=0
                  if FETCH_TITLES and FETCH_TITLES_LIMIT>0:
                      for url in self.all_urls:
                          if fetched>=FETCH_TITLES_LIMIT: break
                          html_text=http_get(url, timeout=FETCH_TIMEOUT_SEC)
                          title=extract_title(html_text)
                          if title:
                              dom=self._domain_of(url)
                              self.data[dom][url]["title"]=title
                              fetched+=1
                  # 2) Resolve label
                  for dom, bucket in self.data.items():
                      for url, info in bucket.items():
                          label=None
                          # Try KeyBERT on combined signal (title + a top context)
                          if LINK_LABEL_AI=="keybert":
                              corpus = " ".join([info.get("title") or ""] + list(info["contexts"])[:1]).strip()
                              if corpus:
                                  label = kb_label(corpus)
                          # Title fallback
                          if not label and info.get("title"):
                              label = compress_to_2_3_words(info["title"])
                          # Context fallback
                          if not label and info["contexts"]:
                              label = derive_from_context(next(iter(info["contexts"])))
                          # Slug fallback
                          if not label:
                              words = slug_to_words(url)
                              if words: label = compress_to_2_3_words(" ".join(words[:4]))
                          # Domain last
                          if not label: label = label_from_domain(dom)
                          # Normalize strictly to <=3 words
                          label = compress_to_2_3_words(label)
                          info["label"]=label

              def render_grouped_html(self):
                  out=[]
                  out.append('''<style>
          .ss3k-links h4{font:600 14px/1.4 system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;margin:12px 0 6px}
          .ss3k-links ul{margin:0 0 16px; padding-left:18px}
          .ss3k-links li{margin:6px 0}
          .ss3k-linkmeta{color:#64748b; font:12px system-ui; margin-left:6px}
          </style>''')
                  out.append('<div class="ss3k-links">')
                  for dom in sorted(self.data.keys()):
                      out.append(f"<h4>{esc(dom)}</h4>")
                      out.append("<ul>")
                      items=[self.data[dom][u] for u in self.all_urls if u in self.data[dom]]
                      for info in items:
                          url = info["url"]; label=info.get("label") or label_from_domain(dom)
                          out.append(
                              f'<li><a href="{esc(url)}" target="_blank" rel="noopener">{esc(label)}</a>'
                              f'<span class="ss3k-linkmeta">— {esc(dom)}</span></li>'
                          )
                      out.append("</ul>")
                  out.append("</div>")
                  return "\\n".join(out)

          def expand_text_with_entities(text, entities):
              text = text or ""
              esc_text = esc(text)
              if entities:
                  urls = entities.get("urls") or []
                  urls_sorted = sorted(urls, key=lambda u: len(u.get("url","")), reverse=True)
                  for u in urls_sorted:
                      short = u.get("url") or ""
                      expanded = u.get("expanded_url") or u.get("unwound_url") or short
                      if not short: continue
                      esc_text = esc_text.replace(esc(short), f'<a href="{esc(expanded)}" target="_blank" rel="noopener">{esc(expanded)}</a>')
              esc_text = re.sub(r'(https?://\\S+)', r'<a href="\\1" target="_blank" rel="noopener">\\1</a>', esc_text)
              return esc_text

          def extract_urls_from_text(text): return re.findall(r'(https?://\\S+)', text or "")

          # ---------- Build threaded tree ----------
          def build_thread_tree(root_id, tweets):
              children=defaultdict(list)
              meta={}; uid_of={}
              for tid, t in (tweets or {}).items():
                  if str(t.get("conversation_id_str") or t.get("conversation_id") or "") != str(root_id): continue
                  parent=t.get("in_reply_to_status_id_str")
                  if not parent: continue
                  created=parse_created_at(t.get("created_at",""))
                  uid=str(t.get("user_id_str") or t.get("user_id") or "")
                  meta[tid]={"created_ts":created,"uid":uid}
                  uid_of[tid]=uid
                  children[str(parent)].append(tid)
              for pid in list(children.keys()):
                  children[pid].sort(key=lambda x: meta.get(x,{}).get("created_ts",0.0))
              roots = children.get(str(root_id), [])
              return roots, children, meta, uid_of

          # ---------- Rendering with flatten + pulses ----------
          CSS = '''
          <style>
          .ss3k-threads{font:15px/1.5 system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif}
          .ss3k-controls{margin:8px 0 12px}
          .ss3k-controls button{font:12px system-ui;padding:6px 10px;border:1px solid #d1d5db;border-radius:8px;background:#fff;cursor:pointer}
          .ss3k-reply{display:flex;gap:10px;padding:10px;border:1px solid #e5e7eb;border-radius:12px;margin:8px 0;background:#fff;position:relative}
          .ss3k-ravatar{width:32px;height:32px;border-radius:50%;background:#e5e7eb;flex:0 0 32px}
          .ss3k-rcontent{flex:1 1 auto}
          .ss3k-rmeta{font-size:12px;color:#64748b;margin-bottom:4px}
          .ss3k-rmeta a{color:#334155;text-decoration:none}
          .ss3k-rname{font-weight:600;color:#0f172a}
          .ss3k-rtext{white-space:pre-wrap;word-break:break-word;margin-top:4px}
          .ss3k-children{margin-left:42px;border-left:2px solid #e5e7eb;padding-left:10px}
          .ss3k-toggle{font:12px system-ui;color:#0ea5e9;cursor:pointer;margin:4px 0 0 42px}
          .ss3k-hidden{display:none}
          @keyframes ss3k-pulse {0%{box-shadow:0 0 0 0 rgba(59,130,246,.7);}70%{box-shadow:0 0 0 10px rgba(59,130,246,0);}100%{box-shadow:0 0 0 0 rgba(59,130,246,0);} }
          .ss3k-reply.pulse { animation: ss3k-pulse 1s ease-out; }
          .ss3k-dot{position:absolute; right:10px; top:10px; width:8px; height:8px; border-radius:50%; background:#60a5fa; opacity:.0}
          .ss3k-reply.pulse .ss3k-dot{opacity:1}
          </style>
          '''.strip()

          JS = r'''
          <script>
          (function(){
            function toggle(el, show){
              if(show===true){ el.classList.remove('ss3k-hidden'); }
              else if(show===false){ el.classList.add('ss3k-hidden'); }
              else { el.classList.toggle('ss3k-hidden'); }
            }
            function expandAll(root){ root.querySelectorAll('.ss3k-children').forEach(c=>c.classList.remove('ss3k-hidden')); }
            function collapseAll(root){ root.querySelectorAll('.ss3k-children').forEach(c=>c.classList.add('ss3k-hidden')); }

            function bind(){
              var root=document.querySelector('.ss3k-threads'); if(!root) return;

              root.querySelectorAll('[data-toggle-for]').forEach(btn=>{
                btn.addEventListener('click', ()=>{
                  var id=btn.getAttribute('data-toggle-for');
                  var box=document.getElementById(id); if(!box) return;
                  toggle(box);
                  var collapsed=box.classList.contains('ss3k-hidden');
                  btn.textContent = collapsed ? btn.getAttribute('data-collapsed-label') : btn.getAttribute('data-expanded-label');
                });
              });
              var exp=root.querySelector('[data-ss3k-expand-all]');
              var col=root.querySelector('[data-ss3k-collapse-all]');
              if(exp) exp.addEventListener('click', ()=>expandAll(root));
              if(col) col.addEventListener('click', ()=>collapseAll(root));

              var audio = document.getElementById('ss3k-audio') || document.querySelector('audio[data-ss3k-player]');
              var items = [].slice.call(root.querySelectorAll('.ss3k-reply[data-trel]')).map(function(el){
                var t = parseFloat(el.getAttribute('data-trel') || 'NaN');
                return {el: el, t: t, fired: false};
              }).filter(function(x){ return isFinite(x.t); }).sort(function(a,b){ return a.t - b.t; });

              if (audio && items.length){
                var last = 0;
                audio.addEventListener('seeked', function(){
                  if ((audio.currentTime||0) < last - 0.5) { items.forEach(function(it){ it.fired = false; }); }
                  last = audio.currentTime || 0;
                });
                audio.addEventListener('timeupdate', function(){
                  var now = audio.currentTime || 0;
                  if (now >= last){
                    for (var i=0;i<items.length;i++){
                      var it = items[i];
                      if (!it.fired && it.t > last && it.t <= now){
                        it.fired = true;
                        it.el.classList.add('pulse');
                        setTimeout(function(el){ el.classList.remove('pulse'); }, 1100, it.el);
                      }
                    }
                  } else {
                    for (var j=0;j<items.length;j++){
                      var jt = items[j];
                      if (Math.abs(jt.t - now) < 0.15){
                        jt.el.classList.add('pulse');
                        setTimeout(function(el){ el.classList.remove('pulse'); }, 1100, jt.el);
                      }
                    }
                  }
                  last = now;
                });
              }
            }
            if(document.readyState!=='loading') bind(); else document.addEventListener('DOMContentLoaded', bind);
          })();
          </script>
          '''.strip()

          def render_node_html(tid, tweets, users, children_map, meta, uid_of, visited, start_epoch, linkdex):
              if tid in visited: return ""
              visited.add(tid)
              t = tweets.get(tid) or {}
              uid = uid_of.get(tid, str(t.get("user_id_str") or t.get("user_id") or ""))
              u   = (users or {}).get(uid, {}) if uid else {}
              name   = u.get("name") or "User"
              handle = u.get("screen_name") or ""
              avatar = (u.get("profile_image_url_https") or u.get("profile_image_url") or "").replace("_normal.","_bigger.")
              created = meta.get(tid,{}).get("created_ts", parse_created_at(t.get("created_at","")))
              time_str = fmt_utc(created)
              trel = None
              if isinstance(start_epoch,(int,float)) and start_epoch:
                  trel = max(0.0, created - start_epoch)

              text_raw = t.get("full_text") or t.get("text") or ""
              ent = t.get("entities") or {}
              text_html = expand_text_with_entities(text_raw, ent)

              # index links with context
              seen=set()
              for uu in (ent.get("urls") or []):
                  expanded = uu.get("expanded_url") or uu.get("unwound_url") or uu.get("url")
                  if expanded and expanded not in seen:
                      seen.add(expanded); linkdex.add(expanded, context=text_raw)
              for raw_u in re.findall(r'(https?://\\S+)', text_raw or ""):
                  if raw_u not in seen:
                      seen.add(raw_u); linkdex.add(raw_u, context=text_raw)

              tweet_id = t.get("id_str") or str(t.get("id") or tid)
              profile_url = f"https://x.com/{handle}" if handle else None
              tweet_url   = f"https://x.com/{handle}/status/{tweet_id}" if handle else f"https://x.com/i/status/{tweet_id}"

              # flatten self-replies chain
              flat_chain=[]
              cur=tweet_id
              while True:
                  kids = children_map.get(cur, [])
                  same = [k for k in kids if uid_of.get(k)==uid and k not in visited]
                  if not same: break
                  k=same[0]
                  flat_chain.append(k); visited.add(k); cur=k

              parts=[]
              data_trel = f' data-trel="{trel:.3f}"' if trel is not None else ""
              parts.append(f'<div class="ss3k-reply"{data_trel}><span class="ss3k-dot" aria-hidden="true"></span>')
              if profile_url:
                  parts.append(f'  <a href="{esc(profile_url)}" target="_blank" rel="noopener"><img class="ss3k-ravatar" src="{esc(avatar)}" alt=""></a>')
              else:
                  parts.append(f'  <img class="ss3k-ravatar" src="{esc(avatar)}" alt="">')
              parts.append('  <div class="ss3k-rcontent">')
              meta_bits=[]
              if profile_url:
                  meta_bits.append(f'<span class="ss3k-rname"><a href="{esc(profile_url)}" target="_blank" rel="noopener">{esc(name)}</a></span>')
              else:
                  meta_bits.append(f'<span class="ss3k-rname">{esc(name)}</span>')
              if handle: meta_bits.append(f' <span>@{esc(handle)}</span>')
              if tweet_url: meta_bits.append(f' · <a href="{esc(tweet_url)}" target="_blank" rel="noopener">{esc(time_str)}</a>')
              parts.append(f'    <div class="ss3k-rmeta">{"".join(meta_bits)}</div>')
              parts.append(f'    <div class="ss3k-rtext">{text_html}</div>')
              parts.append('  </div></div>')

              # render flattened self chain (each as its own top-level block, not nested)
              for child_id in flat_chain:
                  parts.append(render_node_html(child_id, tweets, users, children_map, meta, uid_of, visited, start_epoch, linkdex))

              # other children (from other users) stay collapsible under this node
              other = [k for k in children_map.get(tweet_id, []) if k not in visited]
              if other:
                  box_id = f"ss3k-children-{tweet_id}"
                  collapsed_label = f"Show {len(other)} repl{'y' if len(other)==1 else 'ies'}"
                  expanded_label  = f"Hide repl{'y' if len(other)==1 else 'ies'}"
                  parts.append(
                    f'<div class="ss3k-toggle" role="button" tabindex="0" data-toggle-for="{box_id}" data-collapsed-label="{esc(collapsed_label)}" data-expanded-label="{esc(expanded_label)}">{esc(collapsed_label)}</div>'
                  )
                  parts.append(f'<div class="ss3k-children ss3k-hidden" id="{box_id}">')
                  for kid in other:
                      parts.append(render_node_html(kid, tweets, users, children_map, meta, uid_of, visited, start_epoch, linkdex))
                  parts.append('</div>')

              return "\\n".join(parts)

          def render_thread_html(roots, tweets, users, children_map, meta, uid_of, linkdex, start_epoch):
              out=[CSS, '<div class="ss3k-threads">']
              out.append('<div class="ss3k-controls"><button type="button" data-ss3k-expand-all>Expand all</button> '
                         '<button type="button" data-ss3k-collapse-all>Collapse all</button></div>')
              visited=set()
              for rid in roots:
                  out.append(render_node_html(rid, tweets, users, children_map, meta, uid_of, visited, start_epoch, linkdex))
              out.append('</div>')
              out.append(JS)
              return "\\n".join(out)

          def fetch_conversation(root_id_str, screen_name_hint):
              return fetch_conversation_adaptive(root_id_str, screen_name_hint=screen_name_hint)

          def main():
              if not PURPLE: write_empty(); return
              m=re.search(r"/([^/]+)/status/(\\d+)", PURPLE)
              if not m: write_empty(); return
              screen_name=m.group(1); tid_str=m.group(2)
              tweets, users = fetch_conversation(tid_str, screen_name_hint=screen_name)
              if not tweets: write_empty(); return

              roots, children_map, meta, uid_of = build_thread_tree(tid_str, tweets)
              linkdex=LinkIndex()

              # Build link index from ALL tweets
              for t in (tweets or {}).values():
                  text_raw=(t.get("full_text") or t.get("text") or "")
                  ent=(t.get("entities") or {})
                  seen=set()
                  for uu in (ent.get("urls") or []):
                      expanded=uu.get("expanded_url") or uu.get("unwound_url") or uu.get("url")
                      if expanded and expanded not in seen:
                          seen.add(expanded); linkdex.add(expanded, context=text_raw)
                  for raw in extract_urls_from_text(text_raw):
                      if raw not in seen:
                          seen.add(raw); linkdex.add(raw, context=text_raw)

              # finalize labels (fetch titles + AI)
              linkdex.finalize_labels()

              start_epoch=load_start_epoch(START_PATH)
              html_out  = render_thread_html(roots, tweets, users or {}, children_map, meta, uid_of, linkdex, start_epoch)
              links_out = linkdex.render_grouped_html()

              open(REPLIES_OUT,"w",encoding="utf-8").write(html_out)
              open(LINKS_OUT,"w",encoding="utf-8").write(links_out)

          if __name__=="__main__":
              main()
          PY
          chmod +x ".github/workflows/scripts/replies.py"

      - name: Build VTT and transcript after trim
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' }}
        shell: bash
        env:
          CC_JSONL: ${{ env.CRAWLER_CC }}
          SHIFT_SECS: ${{ steps.detect.outputs.lead || '0' }}
        run: |
          set -euxo pipefail
          if [ -n "${CC_JSONL:-}" ] && [ -s "${CC_JSONL}" ]; then
            CC_JSONL="${CC_JSONL}" ARTDIR="${ARTDIR}" BASE="${BASE}" SHIFT_SECS="${SHIFT_SECS}" \
              python3 ".github/workflows/scripts/gen_vtt.py"
            [ -s "${ARTDIR}/${BASE}.vtt" ] && echo "VTT_PATH=${ARTDIR}/${BASE}.vtt" >> "$GITHUB_ENV" || true
            [ -s "${ARTDIR}/${BASE}_transcript.html" ] && echo "TRANSCRIPT_PATH=${ARTDIR}/${BASE}_transcript.html" >> "$GITHUB_ENV" || true
          else
            # ensure the replies pulse code has a start marker even without captions
            : > "${ARTDIR}/${BASE}.start.txt"
          fi

      - name: Polish transcript (optional)
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.TRANSCRIPT_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          python3 ".github/workflows/scripts/polish_transcript.py" || true
          if [ -s "${ARTDIR}/${BASE}_transcript_polished.html" ]; then
            echo "TRANSCRIPT_PATH=${ARTDIR}/${BASE}_transcript_polished.html" >> "$GITHUB_ENV"
          fi

      - name: VTT via Deepgram fallback
        id: deepgram
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.VTT_PATH == '' && env.DEEPGRAM_API_KEY != '' && github.event.inputs.do_transcript == 'true' && env.MP3_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          curl -sS -X POST \
            -H "Authorization: Token ${DEEPGRAM_API_KEY}" \
            -H "Content-Type: audio/mpeg" \
            --data-binary @"${MP3_PATH}" \
            "https://api.deepgram.com/v1/listen?model=nova-2&smart_format=true&punctuate=true&format=vtt" \
            -o "${ARTDIR}/${BASE}.vtt" || true
          [ -s "${ARTDIR}/${BASE}.vtt" ] && echo "VTT_PATH=${ARTDIR}/${BASE}.vtt" >> "$GITHUB_ENV" || true

      - name: Upload VTT to GCS
        id: upload_vtt
        if: ${{ github.event.inputs.mode != 'attendees_only' && github.event.inputs.mode != 'replies_only' && env.VTT_PATH != '' }}
        shell: bash
        run: |
          set -euxo pipefail
          DEST="gs://${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.vtt"
          RAW="https://storage.googleapis.com/${GCS_BUCKET}/${BUCKET_PREFIX}/${BASE}.vtt"
          PROXY="https://media.chbmp.org/${PREFIX}/${BASE}.vtt"
          gsutil -m cp "${VTT_PATH}" "$DEST"
          if [ "${{ github.event.inputs.make_public }}" = "true" ]; then
            (gsutil acl ch -u AllUsers:R "$DEST" || gsutil iam ch allUsers:objectViewer "gs://${GCS_BUCKET}") || true
          fi
          echo "vtt_raw=${RAW}"     >> "$GITHUB_OUTPUT"
          echo "vtt_proxy=${PROXY}" >> "$GITHUB_OUTPUT"

      - name: Build attendees HTML
        id: attendees
        if: ${{ github.event.inputs.mode != 'replies_only' && steps.crawl.outcome == 'success' && steps.crawl.outputs.as_line != '' }}
        shell: bash
        env:
          CAND: ${{ steps.crawl.outputs.as_line }}
        run: |
          set -euxo pipefail
          OUT_JSON="${ARTDIR}/attendees.json"
          OUT_HTML="${ARTDIR}/attendees.html"
          jq -r '
            def mkp:
              { handle: (.twitter_screen_name // .user_results?.result?.legacy?.screen_name),
                name:   (.display_name       // .user_results?.result?.legacy?.name)
              }
              | select(.handle!=null and .handle!="" )
              | . + { url: ("https://x.com/" + .handle) };
            (.audioSpace // .) as $a
            | ($a.metadata?.creator_results?.result?.legacy?) as $h
            | ($h.screen_name // empty) as $H
            | {
                host:    ( if $H != "" then [ {handle:$H, name:($h.name // ""), url:("https://x.com/" + $H)} ] else [] end ),
                cohosts: ( ($a.participants?.admins   // []) | map(mkp) | map(select(.handle != $H)) | unique_by(.handle) ),
                speakers:( ($a.participants?.speakers // []) | map(mkp) | unique_by(.handle) )
              }
          ' "${CAND}" > "$OUT_JSON" || true
          if [ -s "$OUT_JSON" ]; then
            jq -r '
              def li: "  <li><a href=\"" + (.url//"#") + "\">" + ((.name // "") + " (@" + (.handle // "") + ")") + "</a></li>";
              def section(title; items):
                if (items|length) > 0
                then "<h3>" + title + "</h3>\n<ul>\n" + (items|map(li)|join("\n")) + "\n</ul>\n"
                else ""
                end;
              . as $d
              | section("Host"; $d.host)
              + section( (if ($d.cohosts|length)==1 then "Co-host" else "Co-hosts" end); $d.cohosts)
              + section("Speakers"; $d.speakers)
            ' "$OUT_JSON" > "$OUT_HTML"
            if grep -qi '<li><a ' "$OUT_HTML"; then
              echo "ATTN_HTML=${OUT_HTML}" >> "$GITHUB_ENV"
              echo "ATTENDEES_OK=1"       >> "$GITHUB_ENV"
            fi
          fi

      - name: Scrape replies and shared links (web)
        shell: bash
        env:
          PURPLE_TWEET_URL: ${{ github.event.inputs.purple_tweet_url }}
          # turn on smart labels
          LINK_LABEL_AI: "keybert"
          LINK_LABEL_MODEL: "sentence-transformers/all-MiniLM-L6-v2"
          LINK_LABEL_FETCH_TITLES: "true"
          LINK_LABEL_FETCH_LIMIT: "18"
          LINK_LABEL_TIMEOUT_SEC: "4"
          KEYBERT_TOPN: "8"
          KEYBERT_NGRAM_MIN: "1"
          KEYBERT_NGRAM_MAX: "3"
          KEYBERT_USE_MMR: "true"
          KEYBERT_DIVERSITY: "0.6"
        run: |
          set -euo pipefail
          python3 ".github/workflows/scripts/replies.py" || true
          if [ -s "${ARTDIR}/${BASE}_replies.html" ]; then
            echo "REPLIES_PATH=${ARTDIR}/${BASE}_replies.html" >> "$GITHUB_ENV"
          fi
          if [ -s "${ARTDIR}/${BASE}_links.html" ]; then
            echo "LINKS_PATH=${ARTDIR}/${BASE}_links.html" >> "$GITHUB_ENV"
          fi

      - name: Derive Space title and publish date
        id: meta
        shell: bash
        env:
          AS_LINE: ${{ steps.crawl.outputs.as_line }}
        run: |
          set -euo pipefail
          TTL=""
          if [ -n "${AS_LINE:-}" ] && [ -s "${AS_LINE}" ]; then
            TTL="$(jq -r '(.audioSpace // .) as $a | ($a.metadata.title // $a.metadata.name // .title // "")' "${AS_LINE}" | sed -E 's/^[[:space:]]+|[[:space:]]+$//g')"
          fi
          if [ -z "$TTL" ]; then TTL="${BASE}"; fi
          echo "TTL_FINAL=$TTL" >> "$GITHUB_ENV"

          START_ISO=""
          if [ -n "${AS_LINE:-}" ] && [ -s "${AS_LINE}" ]; then
            MS="$(jq -r '(.audioSpace // .) as $a | ($a.metadata.started_at // $a.metadata.created_at // $a.metadata.start // empty)' "${AS_LINE}")" || true
            if [[ "$MS" =~ ^[0-9]+$ ]]; then
              if [ ${#MS} -gt 10 ]; then SECS=$((MS/1000)); else SECS=$MS; fi
              START_ISO="$(date -u -d "@$SECS" +%Y-%m-%dT%H:%M:%SZ || true)"
            fi
          fi
          if [ -z "$START_ISO" ] && [ -s "${ARTDIR}/${BASE}.start.txt" ]; then
            START_ISO="$(head -n1 "${ARTDIR}/${BASE}.start.txt" | tr -d '\r\n')"
          fi
          if [ -z "$START_ISO" ]; then START_ISO="$(date -u +%Y-%m-%dT%H:%M:%SZ)"; fi
          echo "START_ISO=$START_ISO" >> "$GITHUB_ENV"

      - name: Register assets in WP
        if: ${{ github.event.inputs.mode == '' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' && (steps.upload_mp3.outputs.audio_proxy != '' || steps.upload_mp3.outputs.audio_raw != '') }}
        shell: bash
        env:
          PID:  ${{ github.event.inputs.post_id }}
          AUD:  ${{ steps.upload_mp3.outputs.audio_proxy || steps.upload_mp3.outputs.audio_raw }}
          VTTU: ${{ steps.upload_vtt.outputs.vtt_proxy   || steps.upload_vtt.outputs.vtt_raw }}
        run: |
          set -euo pipefail
          TTL="${TTL_FINAL:-${BASE}}"

          ATH_FILE="${WORKDIR}/empty_attendees.html"; : > "$ATH_FILE"
          [ -n "${ATTN_HTML:-}" ] && [ -s "${ATTN_HTML:-}" ] && ATH_FILE="${ATTN_HTML}"

          TR_FILE="${WORKDIR}/empty_transcript.html"; : > "$TR_FILE"
          [ -n "${TRANSCRIPT_PATH:-}" ] && [ -s "${TRANSCRIPT_PATH}" ] && TR_FILE="${TRANSCRIPT_PATH}"

          REP_FILE="${WORKDIR}/empty_replies.html"; : > "$REP_FILE"
          [ -n "${REPLIES_PATH:-}" ] && [ -s "${REPLIES_PATH}" ] && REP_FILE="${REPLIES_PATH}"

          LNK_FILE="${WORKDIR}/empty_links.html"; : > "$LNK_FILE"
          [ -n "${LINKS_PATH:-}" ] && [ -s "${LINKS_PATH}" ] && LNK_FILE="${LINKS_PATH}"

          RX_FILE="${WORKDIR}/empty_reactions.json"; echo "[]" > "$RX_FILE"
          if [ -s "${ARTDIR}/${BASE}_reactions.json" ]; then
            RX_FILE="${ARTDIR}/${BASE}_reactions.json"
          fi

          REQ="${WORKDIR}/wp_register_body.json"
          jq -n \
            --arg gcs   "${AUD}" \
            --arg mime  "audio/mpeg" \
            --arg pid   "${PID}" \
            --arg ttl   "${TTL}" \
            --arg vtt   "${VTTU}" \
            --arg when  "${START_ISO:-}" \
            --rawfile ath "${ATH_FILE}" \
            --rawfile tr  "${TR_FILE}" \
            --rawfile rep "${REP_FILE}" \
            --rawfile lnk "${LNK_FILE}" \
            --rawfile rx  "${RX_FILE}" \
            '{
               gcs_url: $gcs,
               mime:    $mime,
               post_id: ($pid|tonumber),
               title:   $ttl
             }
             + (if ($when|length)>0 then {post_date_gmt:$when, space_started_at:$when, publish_date:$when} else {} end)
             + (if ($vtt|length)>0 then {vtt_url:$vtt} else {} end)
             + (if ($ath|gsub("\\s";"")|length)>0 then {attendees_html:$ath} else {} end)
             + (if ($tr|gsub("\\s";"")|length)>0 then {transcript:$tr, has_transcript:true} else {} end)
             + (if ($rep|gsub("\\s";"")|length)>0 then {ss3k_replies_html:$rep} else {} end)
             + (if ($lnk|gsub("\\s";"")|length)>0 then {shared_links_html:$lnk} else {} end)
             + (if ($rx|gsub("\\s";"")|length)>2 then {reactions_json:$rx} else {} end)
            ' > "$REQ"

          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" \
            -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/register" \
            --data-binary @"$REQ" | jq -r .

      - name: Patch WP attendees only
        if: ${{ github.event.inputs.mode == 'attendees_only' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' }}
        shell: bash
        run: |
          set -euo pipefail
          AT_HTML=""
          if [ -n "${ATTN_HTML:-}" ] && [ -s "${ATTN_HTML:-}" ]; then AT_HTML="$(cat "${ATTN_HTML}")"; fi
          BODY="$(jq -n --arg pid "${{ github.event.inputs.post_id }}" --arg ath "${AT_HTML}" \
            '{post_id: ($pid|tonumber), status:"complete", progress:100}
             + (if ($ath|length)>0 then {attendees_html:$ath} else {} end)')"
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/patch-assets" -d "$BODY" | jq -r .

      - name: Patch WP replies only
        if: ${{ github.event.inputs.mode == 'replies_only' && env.WP_BASE_URL != '' && env.WP_USER != '' && env.WP_APP_PASSWORD != '' && github.event.inputs.post_id != '' }}
        shell: bash
        run: |
          set -euo pipefail
          REP="$( [ -n "${REPLIES_PATH:-}" ] && [ -s "${REPLIES_PATH:-}" ] && cat "${REPLIES_PATH}" || echo "" )"
          LNK="$( [ -n "${LINKS_PATH:-}" ] && [ -s "${LINKS_PATH:-}" ] && cat "${LINKS_PATH}" || echo "" )"
          BODY="$(jq -n --arg pid "${{ github.event.inputs.post_id }}" --arg rep "${REP}" --arg lnk "${LNK}" \
            '{post_id: ($pid|tonumber), status:"complete", progress:100}
             + (if ($rep|length)>0 then {ss3k_replies_html:$rep} else {} end)
             + (if ($lnk|length)>0 then {shared_links_html:$lnk} else {} end)')"
          curl -sS -u "${WP_USER}:${WP_APP_PASSWORD}" -H "Content-Type: application/json" \
            -X POST "${WP_BASE_URL%/}/wp-json/ss3k/v1/patch-assets" -d "$BODY" | jq -r .

      - name: Summary
        shell: bash
        env:
          SID: ${{ steps.ids.outputs.space_id }}
        run: |
          {
            echo "### Space Worker Summary"
            echo "- Mode ${{ github.event.inputs.mode }}"
            echo "- Space URL ${{ github.event.inputs.space_url }}"
            echo "- Purple URL ${{ github.event.inputs.purple_tweet_url }}"
            echo "- Space ID ${SID}"
            echo "- Post ID ${{ github.event.inputs.post_id }}"
            echo "- Title ${TTL_FINAL:-}"
            echo "- Publish (UTC) ${START_ISO:-}"
            if [ -n "${{ steps.upload_mp3.outputs.audio_proxy }}" ]; then
              echo "- Audio ${{ steps.upload_mp3.outputs.audio_proxy }}"
            elif [ -n "${{ steps.upload_mp3.outputs.audio_raw }}" ]; then
              echo "- Audio ${{ steps.upload_mp3.outputs.audio_raw }}"
            fi
            if [ -n "${{ steps.upload_vtt.outputs.vtt_proxy }}" ]; then
              echo "- VTT ${{ steps.upload_vtt.outputs.vtt_proxy }}"
            elif [ -n "${{ steps.upload_vtt.outputs.vtt_raw }}" ]; then
              echo "- VTT ${{ steps.upload_vtt.outputs.vtt_raw }}"
            fi
          } >> "$GITHUB_STEP_SUMMARY"
